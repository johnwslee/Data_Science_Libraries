{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63311384-ad9d-4b2b-85ff-e0be3f34ac24",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0992aff6-45b8-44fc-a8c2-8f96859a215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from skimage.transform import resize   # Image Processing Library\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc    # Garbage Collector\n",
    "gc.collect()\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "# Pytorch\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "from vocabulary.vocabulary import Vocabulary\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import json\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d7b937-694c-4647-872d-ef95cb5080af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['info.json', 'raw', 'test', 'train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../../../fiftyone/coco-2014\" directory.\n",
    "print(os.listdir(\"../../../fiftyone/coco-2014\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce269b3d-0436-47ad-baad-9cb8fa25918c",
   "metadata": {},
   "source": [
    "# Preparation of Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8d88c6-3c57-4777-a31b-aff439beda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Datasets using Custom Images\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, transform, mode, batch_size, vocab_threshold, \n",
    "                 vocab_file, start_word, end_word, unk_word, annotations_file, \n",
    "                 vocab_from_file, img_folder):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word, \n",
    "                                end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == 'train':\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print('Obtaining caption lengths...')\n",
    "            all_tokens = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]]['caption']).lower()\n",
    "                ) for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == 'train':\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where(\n",
    "            [self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4197522d-2ab9-4b1a-82e5-02f74aed87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(transform,\n",
    "               mode='train',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"<start>\",\n",
    "               end_word=\"<end>\",\n",
    "               unk_word=\"<unk>\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               cocoapi_loc='/opt'):\n",
    "    \"\"\"Returns the data loader.\n",
    "    Args:\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/train2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/test2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/image_info_test2014.json')\n",
    "\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(transform=transform,\n",
    "                          mode=mode,\n",
    "                          batch_size=batch_size,\n",
    "                          vocab_threshold=vocab_threshold,\n",
    "                          vocab_file=vocab_file,\n",
    "                          start_word=start_word,\n",
    "                          end_word=end_word,\n",
    "                          unk_word=unk_word,\n",
    "                          annotations_file=annotations_file,\n",
    "                          vocab_from_file=vocab_from_file,\n",
    "                          img_folder=img_folder)\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False))\n",
    "    else:\n",
    "        data_loader = data.DataLoader(dataset=dataset,\n",
    "                                      batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecc314-cbf1-47a5-a60d-d13b1bdce0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
