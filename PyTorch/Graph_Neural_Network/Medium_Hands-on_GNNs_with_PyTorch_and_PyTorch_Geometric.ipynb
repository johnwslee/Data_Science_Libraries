{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bdc60d1-5990-4125-ac0f-914e822b3a19",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/towards-data-science/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8) in Medium and [this Github repo](https://github.com/khuangaf/Pytorch-Geometric-YooChoose)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d34bc-2db5-4098-9e10-f33cb80683d9",
   "metadata": {},
   "source": [
    "You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).\n",
    "\n",
    "In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.\n",
    "\n",
    "Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.\n",
    "\n",
    "Given its advantage in speed and convenience, without a doubt, PyG is one of the most popular and widely used GNN libraries. Let’s dive into the topic and get our hands dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9d539-870d-4bdb-a3c9-9d9975f97aa8",
   "metadata": {},
   "source": [
    "# 1. PyTorch Geometric Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae4ad45-acc9-417b-a346-f9c8fd6078f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d1c2d-c0cb-400f-9b89-cb78085de91f",
   "metadata": {},
   "source": [
    "This section will walk you through the basics of PyG. Essentially, it will cover `torch_geometric.data` and `torch_geometric.nn`. You will learn how to pass geometric data into your GNN, and how to design a custom MessagePassing layer, the core of GNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd8d12-fd7e-485b-890b-d70386099825",
   "metadata": {},
   "source": [
    "## 1.1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459312b-e194-48b5-b4af-855168abc98b",
   "metadata": {},
   "source": [
    "The `torch_geometric.data` module contains a `Data` class that allows you to create graphs from your data very easily. You only need to specify:\n",
    "\n",
    "1. the attributes/ features associated with each node\n",
    "2. the connectivity/adjacency of each node (edge index)\n",
    "\n",
    "Let’s use the following graph to demonstrate how to create a Data object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792a289-5335-4342-8b73-b17f78ca0d1c",
   "metadata": {},
   "source": [
    "<img src=\"example_graph.webp\" style=\"width:400px;height:300px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41df0a-28f8-4a37-91fb-b001ada2229a",
   "metadata": {},
   "source": [
    "So there are 4 nodes in the graph, v0 … v3, each of which is associated with a 2-dimensional feature vector, and a label y indicating its class. These two can be represented as FloatTensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c7e104-1911-42b3-86d4-1d7cda7711d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "x = torch.tensor([[2, 1], [5, 6], [3, 7], [12, 0]], dtype=torch.float)\n",
    "# Class\n",
    "y = torch.tensor([0, 1, 0, 1], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49184fb-7aea-4fac-a695-8146fee36536",
   "metadata": {},
   "source": [
    "The graph connectivity (edge index) should be confined with the COO format, i.e. the first list contains the index of the source nodes, while the index of target nodes is specified in the second list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af67725c-3a3a-4fb6-a450-68fea57a53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 2, 0, 3], [1, 0, 1, 3, 2]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39e8eb-3ff2-4be3-888f-f8f1705b6f9a",
   "metadata": {},
   "source": [
    "Note that the order of the edge index is irrelevant to the Data object you create since such information is only for computing the adjacency matrix. Therefore, the above edge_index express the same information as the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a447979f-caf8-468f-a5d9-1ac2059f410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 2, 1, 0, 3], [3, 1, 0, 1, 2]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a23152-77bd-4f54-b1e0-36ffe82214ee",
   "metadata": {},
   "source": [
    "Putting them together, we can create a `Data` object as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6da546-f8dc-43f8-902c-b9439abd5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=x, y=y, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8794f759-5237-49cb-ac08-b43bf9d01feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4, 2], edge_index=[2, 5], y=[4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af3540-1258-4e57-a34d-a0996e2a0438",
   "metadata": {},
   "source": [
    "## 1.2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e3a7b-fb43-4617-a3a7-05ac46b36f6c",
   "metadata": {},
   "source": [
    "The dataset creation procedure is not very straightforward, but it may seem familiar to those who’ve used torchvision, as PyG is following its convention. PyG provides two different types of dataset classes, **InMemoryDataset** and **Dataset**. As they indicate literally, the former one is for data that fit in your RAM, while the second one is for much larger data. Since their implementations are quite similar, I will only cover InMemoryDataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88bbf7d-3e99-4706-9032-2a2e2c8c5e14",
   "metadata": {},
   "source": [
    "To create an InMemoryDataset object, there are 4 functions you need to implement:\n",
    "\n",
    "- `raw_file_names()`\n",
    "\n",
    "It returns a list that shows a list of raw, unprocessed file names. If you only have a file then the returned list should only contain 1 element. In fact, you can simply return an empty list and specify your file later in process().\n",
    "\n",
    "- `processed_file_names()`\n",
    "\n",
    "Similar to the last function, it also returns a list containing the file names of all the processed data. After process() is called, Usually, the returned list should only have one element, storing the only processed data file name.\n",
    "\n",
    "- `download()`\n",
    "\n",
    "This function should download the data you are working on to the directory as specified in `self.raw_dir`. If you don’t need to download data, simply drop in `pass` in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ac68a-bbef-41fe-b7d3-b3582db0048e",
   "metadata": {},
   "source": [
    "- `process()`\n",
    "\n",
    "This is the **most important method of Dataset**. You need to gather your data into a list of Data objects. Then, call `self.collate()` to compute the slices that will be used by the DataLoader object. The following shows an example of the custom dataset. The following shows an example of the custom dataset from [PyG official website](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657d5cfe-e68e-4021-a2f4-12912488f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"some_file_1\", \"some_file_2\", ...]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"data.pt\"]\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        download_url(url, self.raw_dir)\n",
    "        ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = [...]\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        # Because saving a huge python list is really slow, we collate the list into one huge\n",
    "        # torch_geometric.data.Data object via torch_geometric.data.InMemoryDataset.collate()\n",
    "        # before saving . The collated data object has concatenated all examples into one big data object\n",
    "        # and, in addition, returns a slices dictionary to reconstruct single examples from this object.\n",
    "        # Finally, we need to load these two objects in the constructor into the properties self.data\n",
    "        # and self.slices.\n",
    "\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02863ed7-5d46-4d21-86b1-682ee6ce5234",
   "metadata": {},
   "source": [
    "## 1.3. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903b8a4-7901-4e74-97d4-5150ad901389",
   "metadata": {},
   "source": [
    "The `DataLoader` class allows you to feed data by batch into the model effortlessly. To create a `DataLoader` object, you simply specify the `Dataset` and the `batch_size` you want."
   ]
  },
  {
   "cell_type": "raw",
   "id": "87a10081-5d9a-48d2-afbe-57c536c7417b",
   "metadata": {},
   "source": [
    "loader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b5f78-2447-4fca-a0a7-fd04c3e32299",
   "metadata": {},
   "source": [
    "Every iteration of a `DataLoader` object yields a Batch object, which is very much like a `Data` object but with an attribute, “batch”. It indicates which graph each node is associated with. Since a `DataLoader` aggregates `x`, `y`, and `edge_index` from different samples/ graphs into Batches, the GNN model needs this “batch” information to know which nodes belong to the same graph within a batch to perform computation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d1f2563-2377-4ece-8429-737e79c0d4eb",
   "metadata": {},
   "source": [
    "for batch in loader:\n",
    "    batch\n",
    "    >>> Batch(x=[1024, 21], edge_index=[2, 1568], y=[512], batch=[1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e540ad4-57b2-49ec-9d24-7c22b571d043",
   "metadata": {},
   "source": [
    "## 1.4. MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83931ca1-b7c1-4075-9e33-1aa0c3e8acd1",
   "metadata": {},
   "source": [
    "Message passing is the essence of GNN which describes how node embeddings are learned. I have talked about in my last post, so I will just briefly run through this with terms that conform to the PyG documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb06e4-1baf-4ea5-975e-02415d069db7",
   "metadata": {},
   "source": [
    "<img src=\"message_passing.webp\" style=\"width:500px;height:50px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15884df7-2eed-4d17-a71d-4091cefe2c18",
   "metadata": {},
   "source": [
    "`x` denotes the node embeddings, `e` denotes the edge features, `𝜙` denotes the message function, `□` denotes the aggregation function, `𝛾` denotes the update function. If the edges in the graph have no feature other than connectivity, `e` is essentially the edge index of the graph. The superscript represents the index of the layer. When `k=1`, `x` represents the input feature of each node. Below I will illustrate how each function works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b98e8d-b86a-4ba6-bbf1-19eab64d6285",
   "metadata": {},
   "source": [
    "- `propagate(edge_index, size=None, **kwargs)`:\n",
    "It takes in edge index and other optional information, such as node features (embedding). Calling this function will consequently call message and update.\n",
    "\n",
    "- `message(**kwargs)`:\n",
    "You specify how you construct “message” for each of the node pair (x_i, x_j). Since it follows the calls of propagate, it can take any argument passing to propagate. One thing to note is that you can define the mapping from arguments to the specific nodes with “_i” and “_j”. Therefore, you must be very careful when naming the argument of this function.\n",
    "\n",
    "- `update(aggr_out, **kwargs)`\n",
    "It takes in the aggregated message and other arguments passed into propagate, assigning a new embedding value for each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716178a3-ef66-443c-987a-c192f6184a07",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91204776-d5dd-4b9c-ba21-8baadb132305",
   "metadata": {},
   "source": [
    "Let’s see how we can implement a SageConv layer from the paper [“Inductive Representation Learning on Large Graphs”](https://arxiv.org/abs/1706.02216). The message passing formula of SageConv is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38369c36-164d-450f-b6ce-c7b00964c767",
   "metadata": {},
   "source": [
    "<img src=\"equ_1.webp\" style=\"width:500px;height:100px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625eab2c-bed0-4357-81f7-a412c299e458",
   "metadata": {},
   "source": [
    "Here, we use max pooling as the aggregation method. Therefore, the right-hand side of the first line can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5bd97-6cf8-41ed-8273-df3a5b3057be",
   "metadata": {},
   "source": [
    "<img src=\"equ_2.webp\" style=\"width:500px;height:50px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161ffd4-d60b-4218-b419-434adca8a510",
   "metadata": {},
   "source": [
    "which illustrates how the “message” is constructed. Each neighboring node embedding is multiplied by a weight matrix, added a bias and passed through an activation function. This can be easily done with `torch.nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43d5c68-bc66-40d5-b442-1965a2ffd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__(aggr=\"max\")\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        x_j = self.lin(x_j)\n",
    "        x_j = self.act(x_j)\n",
    "\n",
    "        return x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5b022-27c1-469f-bdf5-f08a1ea42195",
   "metadata": {},
   "source": [
    "As for the update part, the aggregated message and the current node embedding is aggregated. Then, it is multiplied by another weight matrix and applied another activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d7e493-e6f5-4caa-80f2-f87ade0e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__(aggr=\"max\")\n",
    "        self.update_lin = torch.nn.Linear(\n",
    "            in_channels + out_channels, in_channels, bias=False\n",
    "        )\n",
    "        self.update_act = torch.nn.ReLU()\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
    "        new_embedding = self.update_lin(new_embedding)\n",
    "        new_embedding = torch.update_act(new_embedding)\n",
    "\n",
    "        return new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f183a7-fdc0-48ea-94d9-9b0cbae8255b",
   "metadata": {},
   "source": [
    "Putting it together, we have the following SageConv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c84df6c-9a70-472a-9a92-c46969f60804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch.nn import Sequential as Seq\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5a79c1-2b39-42f4-bfb6-5416c00ad7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__(aggr=\"max\")  # \"Max\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.update_lin = torch.nn.Linear(\n",
    "            in_channels + out_channels, in_channels, bias=False\n",
    "        )\n",
    "        self.update_act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        x_j = self.lin(x_j)\n",
    "        x_j = self.act(x_j)\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
    "\n",
    "        new_embedding = self.update_lin(new_embedding)\n",
    "        new_embedding = self.update_act(new_embedding)\n",
    "\n",
    "        return new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2afa11-3d1f-44f6-9d66-6a68bea220a6",
   "metadata": {},
   "source": [
    "# 2. A Real-World Example — RecSys Challenge 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a0a15-a023-4a36-9f39-0bc9e127b208",
   "metadata": {},
   "source": [
    "The RecSys Challenge 2015 is challenging data scientists to build a session-based recommender system. Participants in this challenge are asked to solve two tasks:\n",
    "\n",
    "1. Predict whether there will be a buy event followed by a sequence of clicks\n",
    "2. Predict which item will be bought\n",
    "\n",
    "First, we download the data from the official website of RecSys Challenge 2015 and construct a Dataset. We’ll start with the first task as that one is easier.\n",
    "\n",
    "The challenge provides two main sets of data, *yoochoose-clicks.dat*, and *yoochoose-buys.dat*, containing click events and buy events, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb685c-ddc1-431c-8033-137df32f91c8",
   "metadata": {},
   "source": [
    "## 2.1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649556ca-cafc-4b22-8e27-38e6ecbb820c",
   "metadata": {},
   "source": [
    "After downloading the data, we preprocess it so that it can be fed to our model. `item_ids` are categorically encoded to ensure the encoded `item_ids`, which will later be mapped to an embedding matrix, starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12ebe6a-1167-438f-897a-ae9da4b4447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c76d33-f7c8-4894-8608-9e8167246449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\AppData\\Local\\Temp\\ipykernel_31008\\2078561977.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/yoochoose-clicks.dat\", header=None)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/yoochoose-clicks.dat\", header=None)\n",
    "df.columns = [\"session_id\", \"timestamp\", \"item_id\", \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618884c6-892b-4f4d-a3a4-5b7d0421900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:51:09.277Z</td>\n",
       "      <td>214536502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:09.868Z</td>\n",
       "      <td>214536500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:46.998Z</td>\n",
       "      <td>214536506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:57:00.306Z</td>\n",
       "      <td>214577561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07T13:56:37.614Z</td>\n",
       "      <td>214662742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003939</th>\n",
       "      <td>11299809</td>\n",
       "      <td>2014-09-25T09:33:22.412Z</td>\n",
       "      <td>214819412</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003940</th>\n",
       "      <td>11299809</td>\n",
       "      <td>2014-09-25T09:43:52.821Z</td>\n",
       "      <td>214830939</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003941</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:09.741Z</td>\n",
       "      <td>214854855</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003942</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:11.894Z</td>\n",
       "      <td>214854838</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003943</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:25.146Z</td>\n",
       "      <td>214848658</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33003944 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id                 timestamp    item_id category\n",
       "0                  1  2014-04-07T10:51:09.277Z  214536502        0\n",
       "1                  1  2014-04-07T10:54:09.868Z  214536500        0\n",
       "2                  1  2014-04-07T10:54:46.998Z  214536506        0\n",
       "3                  1  2014-04-07T10:57:00.306Z  214577561        0\n",
       "4                  2  2014-04-07T13:56:37.614Z  214662742        0\n",
       "...              ...                       ...        ...      ...\n",
       "33003939    11299809  2014-09-25T09:33:22.412Z  214819412        S\n",
       "33003940    11299809  2014-09-25T09:43:52.821Z  214830939        S\n",
       "33003941    11299811  2014-09-24T19:02:09.741Z  214854855        S\n",
       "33003942    11299811  2014-09-24T19:02:11.894Z  214854838        S\n",
       "33003943    11299811  2014-09-24T19:02:25.146Z  214848658        S\n",
       "\n",
       "[33003944 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c39c2b27-a15b-4b3d-8dcc-06ab41ac9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df = pd.read_csv(\"data/yoochoose-buys.dat\", header=None)\n",
    "buy_df.columns = [\"session_id\", \"timestamp\", \"item_id\", \"price\", \"quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9635ff64-2019-48da-845b-2955d4dbd96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420374</td>\n",
       "      <td>2014-04-06T18:44:58.314Z</td>\n",
       "      <td>214537888</td>\n",
       "      <td>12462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420374</td>\n",
       "      <td>2014-04-06T18:44:58.325Z</td>\n",
       "      <td>214537850</td>\n",
       "      <td>10471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281626</td>\n",
       "      <td>2014-04-06T09:40:13.032Z</td>\n",
       "      <td>214535653</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420368</td>\n",
       "      <td>2014-04-04T06:13:28.848Z</td>\n",
       "      <td>214530572</td>\n",
       "      <td>6073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420368</td>\n",
       "      <td>2014-04-04T06:13:28.858Z</td>\n",
       "      <td>214835025</td>\n",
       "      <td>2617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150748</th>\n",
       "      <td>11368701</td>\n",
       "      <td>2014-09-26T07:52:51.357Z</td>\n",
       "      <td>214849809</td>\n",
       "      <td>554</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150749</th>\n",
       "      <td>11368691</td>\n",
       "      <td>2014-09-25T09:37:44.206Z</td>\n",
       "      <td>214700002</td>\n",
       "      <td>6806</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150750</th>\n",
       "      <td>11523941</td>\n",
       "      <td>2014-09-25T06:14:47.965Z</td>\n",
       "      <td>214578011</td>\n",
       "      <td>14556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150751</th>\n",
       "      <td>11423202</td>\n",
       "      <td>2014-09-26T18:49:34.024Z</td>\n",
       "      <td>214849164</td>\n",
       "      <td>1046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150752</th>\n",
       "      <td>11423202</td>\n",
       "      <td>2014-09-26T18:49:34.026Z</td>\n",
       "      <td>214560500</td>\n",
       "      <td>5549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1150753 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         session_id                 timestamp    item_id  price  quantity\n",
       "0            420374  2014-04-06T18:44:58.314Z  214537888  12462         1\n",
       "1            420374  2014-04-06T18:44:58.325Z  214537850  10471         1\n",
       "2            281626  2014-04-06T09:40:13.032Z  214535653   1883         1\n",
       "3            420368  2014-04-04T06:13:28.848Z  214530572   6073         1\n",
       "4            420368  2014-04-04T06:13:28.858Z  214835025   2617         1\n",
       "...             ...                       ...        ...    ...       ...\n",
       "1150748    11368701  2014-09-26T07:52:51.357Z  214849809    554         2\n",
       "1150749    11368691  2014-09-25T09:37:44.206Z  214700002   6806         5\n",
       "1150750    11523941  2014-09-25T06:14:47.965Z  214578011  14556         1\n",
       "1150751    11423202  2014-09-26T18:49:34.024Z  214849164   1046         1\n",
       "1150752    11423202  2014-09-26T18:49:34.026Z  214560500   5549         1\n",
       "\n",
       "[1150753 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9248afa7-068b-42a2-a93c-e30b5e2fecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582c0f46-5369-4d1c-bff5-3afae053713c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:51:09.277Z</td>\n",
       "      <td>2053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:09.868Z</td>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:46.998Z</td>\n",
       "      <td>2054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:57:00.306Z</td>\n",
       "      <td>9876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07T13:56:37.614Z</td>\n",
       "      <td>19448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003939</th>\n",
       "      <td>11299809</td>\n",
       "      <td>2014-09-25T09:33:22.412Z</td>\n",
       "      <td>39347</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003940</th>\n",
       "      <td>11299809</td>\n",
       "      <td>2014-09-25T09:43:52.821Z</td>\n",
       "      <td>42548</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003941</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:09.741Z</td>\n",
       "      <td>50432</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003942</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:11.894Z</td>\n",
       "      <td>50425</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003943</th>\n",
       "      <td>11299811</td>\n",
       "      <td>2014-09-24T19:02:25.146Z</td>\n",
       "      <td>48543</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33003944 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id                 timestamp  item_id category\n",
       "0                  1  2014-04-07T10:51:09.277Z     2053        0\n",
       "1                  1  2014-04-07T10:54:09.868Z     2052        0\n",
       "2                  1  2014-04-07T10:54:46.998Z     2054        0\n",
       "3                  1  2014-04-07T10:57:00.306Z     9876        0\n",
       "4                  2  2014-04-07T13:56:37.614Z    19448        0\n",
       "...              ...                       ...      ...      ...\n",
       "33003939    11299809  2014-09-25T09:33:22.412Z    39347        S\n",
       "33003940    11299809  2014-09-25T09:43:52.821Z    42548        S\n",
       "33003941    11299811  2014-09-24T19:02:09.741Z    50432        S\n",
       "33003942    11299811  2014-09-24T19:02:11.894Z    50425        S\n",
       "33003943    11299811  2014-09-24T19:02:25.146Z    48543        S\n",
       "\n",
       "[33003944 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"item_id\"] = item_encoder.fit_transform(df.item_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71846508-cfe7-4ea1-91ee-4f3c9588f1d5",
   "metadata": {},
   "source": [
    "Since the data is quite large, we subsample it for easier demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77efe35-15f5-494c-9f85-b05f066c7931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id    1000000\n",
       "timestamp     3561375\n",
       "item_id         35755\n",
       "category          237\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample a couple of them\n",
    "sampled_session_id = np.random.choice(df.session_id.unique(), 1000000, replace=False)\n",
    "df = df.loc[df.session_id.isin(sampled_session_id)]\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95f408-1ac3-4b8c-948a-473734b6e026",
   "metadata": {},
   "source": [
    "To determine the ground truth, i.e. whether there is any buy event for a given session, we simply check if a session_id in yoochoose-clicks.dat presents in yoochoose-buys.dat as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d77831-7616-49da-a4d9-081ae58699e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\AppData\\Local\\Temp\\ipykernel_31008\\3350967650.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"label\"] = df.session_id.isin(buy_df.session_id)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-02T13:17:46.940Z</td>\n",
       "      <td>28989</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-02T13:26:02.515Z</td>\n",
       "      <td>35310</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-02T13:30:12.318Z</td>\n",
       "      <td>43178</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>23</td>\n",
       "      <td>2014-04-04T07:43:18.598Z</td>\n",
       "      <td>29373</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>23</td>\n",
       "      <td>2014-04-04T07:49:04.634Z</td>\n",
       "      <td>29373</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_id                 timestamp  item_id category  label\n",
       "10           3  2014-04-02T13:17:46.940Z    28989        0  False\n",
       "11           3  2014-04-02T13:26:02.515Z    35310        0  False\n",
       "12           3  2014-04-02T13:30:12.318Z    43178        0  False\n",
       "61          23  2014-04-04T07:43:18.598Z    29373        0  False\n",
       "62          23  2014-04-04T07:49:04.634Z    29373        0  False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"] = df.session_id.isin(buy_df.session_id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d826eb2-8e86-4d3f-ab12-ebf67f224701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    3208103\n",
       "True      354202\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087f668-e6d4-4c30-be7f-8a0545304863",
   "metadata": {},
   "source": [
    "## 2.2. Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93578db4-88c1-4974-b3f7-b799e7df1a1f",
   "metadata": {},
   "source": [
    "The data is ready to be transformed into a Dataset object after the preprocessing step. Here, we treat each item in a session as a node, and therefore all items in the same session form a graph. To build the dataset, we group the preprocessed data by session_id and iterate over these groups. In each iteration, the item_id in each group are categorically encoded again since for each graph, the node index should count from 0. Thus, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce6481d6-cceb-4f39-929d-d982951e4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a303cfb-32bb-4522-be88-93fe175a1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseBinaryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(YooChooseBinaryDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"yoochoose_click_binary_1M_sess.dataset\"]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "\n",
    "        # process by session_id\n",
    "        grouped = df.groupby(\"session_id\")\n",
    "        for session_id, group in tqdm(grouped):\n",
    "            sess_item_id = LabelEncoder().fit_transform(group.item_id)\n",
    "            group = group.reset_index(drop=True)\n",
    "            group[\"sess_item_id\"] = sess_item_id\n",
    "            node_features = (\n",
    "                group.loc[group.session_id == session_id, [\"sess_item_id\", \"item_id\"]]\n",
    "                .sort_values(\"sess_item_id\")\n",
    "                .item_id.drop_duplicates()\n",
    "                .values\n",
    "            )\n",
    "\n",
    "            node_features = torch.LongTensor(node_features).unsqueeze(1)\n",
    "            target_nodes = group.sess_item_id.values[1:]\n",
    "            source_nodes = group.sess_item_id.values[:-1]\n",
    "\n",
    "            edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "            x = node_features\n",
    "\n",
    "            y = torch.FloatTensor([group.label.values[0]])\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ce64d-d807-47f7-a40d-ff26b280fc4f",
   "metadata": {},
   "source": [
    "After building the dataset, we call `shuffle()` to make sure it has been randomly shuffled and then split it into three sets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa0f9d0-8659-4b34-928c-0169257741b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YooChooseBinaryDataset(root=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54638645-67f8-461d-94d9-bd9b07f85ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 100000, 100000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset[:800000]\n",
    "val_dataset = dataset[800000:900000]\n",
    "test_dataset = dataset[900000:]\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e25914-cf70-4a21-8a9f-aac3ada22634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 2], y=[1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c4603-b5e6-4d1b-819f-37df0422a991",
   "metadata": {},
   "source": [
    "## 2.3. Build a Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0eb942-f418-42d7-bd20-e6e039e10f7d",
   "metadata": {},
   "source": [
    "The following custom GNN takes reference from one of the examples in PyG’s official Github repository. I changed the GraphConv layer with our self-implemented SAGEConv layer illustrated above. In addition, the output layer was also modified to match with a binary classification setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33386a48-6606-4e7d-ba59-591cfa967497",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30085f11-fa15-4dfd-a7dc-d0408736cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d55c24-23d3-4f53-adcd-3cb328957fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(embed_dim, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = SAGEConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = SAGEConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "        self.item_embedding = torch.nn.Embedding(\n",
    "            num_embeddings=df.item_id.max() + 1, embedding_dim=embed_dim\n",
    "        )\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec0bc8-742e-4583-add7-c810dca9d4c2",
   "metadata": {},
   "source": [
    "## 2.4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae609b-bccc-4f22-84e8-120604dcc7fe",
   "metadata": {},
   "source": [
    "Training our custom GNN is very easy, we simply iterate the DataLoader constructed from the training set and back-propagate the loss function. Here, we use Adam as the optimizer with the learning rate set to 0.005 and Binary Cross Entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91917803-4457-4cf6-8220-a5766432e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e4fcbb8-171a-4b97-b81d-8b7b4d0156e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        label = data.y.to(device)\n",
    "        loss = crit(output, label)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40cea469-3442-4bbf-8099-ff393467fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96cc3908-2cd7-457f-b243-7bea2c73840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "crit = torch.nn.BCELoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6e884-d362-4a9f-a19d-4fc74a2df050",
   "metadata": {},
   "source": [
    "## 2.5. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d341729-12b8-4c1b-a2f6-b52da4a3c26e",
   "metadata": {},
   "source": [
    "This label is highly unbalanced with an overwhelming amount of negative labels since most of the sessions are not followed by any buy event. In other words, a dumb model guessing all negatives would give you above 90% accuracy. Therefore, instead of accuracy, Area Under Curve (AUC) is a better metric for this task as it only cares if the positive examples are scored higher than the negative examples. We use the off-the-shelf AUC calculation function from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af2484f-3cf6-407f-b7ee-72914b1dc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fec10752-5892-4866-a7b5-4df857885e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            pred = model(data).detach().cpu().numpy()\n",
    "\n",
    "            label = data.y.detach().cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            labels.append(label)\n",
    "\n",
    "    predictions = np.hstack(predictions)\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    return roc_auc_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fe250-e58c-4bee-ad94-5a5262b25700",
   "metadata": {},
   "source": [
    "## 2.6. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f162137-a490-494d-a09f-91889048d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b870084c-f85a-4763-a8e4-a441674a3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2508fc28-c963-4c67-aaba-7c5bf4458e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.20935, Train Auc: 0.73691, Val Auc: 0.72700, Test Auc: 0.69942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.19090, Train Auc: 0.77644, Val Auc: 0.73261, Test Auc: 0.69701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.18170, Train Auc: 0.79200, Val Auc: 0.73049, Test Auc: 0.68921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.17503, Train Auc: 0.80633, Val Auc: 0.72338, Test Auc: 0.67858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.16821, Train Auc: 0.80995, Val Auc: 0.72259, Test Auc: 0.67965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.16320, Train Auc: 0.82540, Val Auc: 0.71272, Test Auc: 0.67124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.15854, Train Auc: 0.83057, Val Auc: 0.70657, Test Auc: 0.66338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.15418, Train Auc: 0.84155, Val Auc: 0.69290, Test Auc: 0.65316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.14954, Train Auc: 0.83834, Val Auc: 0.68186, Test Auc: 0.64070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.14703, Train Auc: 0.84243, Val Auc: 0.67614, Test Auc: 0.63447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.14428, Train Auc: 0.84583, Val Auc: 0.67663, Test Auc: 0.63867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 0.14210, Train Auc: 0.85269, Val Auc: 0.67275, Test Auc: 0.63442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 0.14031, Train Auc: 0.85781, Val Auc: 0.66431, Test Auc: 0.63264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.13942, Train Auc: 0.86169, Val Auc: 0.67141, Test Auc: 0.63711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 0.13816, Train Auc: 0.86897, Val Auc: 0.65952, Test Auc: 0.62737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.13683, Train Auc: 0.86566, Val Auc: 0.66992, Test Auc: 0.63191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 0.13541, Train Auc: 0.87268, Val Auc: 0.65312, Test Auc: 0.62646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 0.13353, Train Auc: 0.87514, Val Auc: 0.65024, Test Auc: 0.62247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 0.13171, Train Auc: 0.87840, Val Auc: 0.65044, Test Auc: 0.61964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 0.12978, Train Auc: 0.87814, Val Auc: 0.65587, Test Auc: 0.62458\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    loss = train()\n",
    "    train_acc = evaluate(train_loader)\n",
    "    val_acc = evaluate(val_loader)\n",
    "    test_acc = evaluate(test_loader)\n",
    "    print(\n",
    "        \"Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}\".format(\n",
    "            epoch, loss, train_acc, val_acc, test_acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4af43e-d64b-49ae-9fa9-cc5c13ab437f",
   "metadata": {},
   "source": [
    "With only 1 Million rows of training data (around 10% of all data) and 1 epoch of training, we can obtain an AUC score of around 0.73 for validation and test set. The score is very likely to improve if more data is used to train the model with larger training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d083409-0c31-4c97-9acd-10a13076320b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
