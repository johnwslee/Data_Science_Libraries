{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b542e6-46d8-4486-8d19-8b8f2277be01",
   "metadata": {},
   "source": [
    "The following is [this article](https://medium.com/towards-data-science/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc) in Medium and [this Github repo](https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Chat_With_Any_Document.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f417760-3866-4738-9fdb-2af5cf2e2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "from filetype import guess\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0187f886-47f9-425a-9112-1367c924181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8953e5-a4db-4d07-8b14-80b765b37dbd",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700a982-457a-4f42-a7ca-cf9b56a397dd",
   "metadata": {},
   "source": [
    "So much valuable information is trapped in PDF and image files. Luckily, we have these powerful brains capable of processing those files to find specific information, which in fact is great.\n",
    "\n",
    "But how many of us, deep inside wouldn’t like to have a tool that can answer any question about a given document?\n",
    "\n",
    "That is the whole purpose of this article. I will explain step-by-step how to build a system that can chat with any PDFs and image files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b57e26-8b0e-4549-b0d7-594b79de0b9b",
   "metadata": {},
   "source": [
    "# General Workflow of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe68e54-9164-43a5-92f6-794c683d2cd1",
   "metadata": {},
   "source": [
    "It’s always good to have a clear understanding of the main components of the system being built. So let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7cfa3-574d-40f4-89fa-1c7d105bbe50",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure_1.webp\" style=\"width:700px;height:800px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1e24-9248-4e4f-a35b-f800c41a8ad2",
   "metadata": {},
   "source": [
    "- First, the user submits the document to be processed, which can be in PDF or image format.\n",
    "- A second module is used to detect the format of the file so that the relevant content extraction function is applied.\n",
    "- The content of the document is then split into multiple chunks using the `Data Splitter` module.\n",
    "- Those chunks are finally transformed into embeddings using the `Chunk Transformer` before they are stored in the vector store.\n",
    "- At the end of the process, the user’s query is used to find relevant chunks containing the answer to that query, and the result is returned as a JSON to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da76a19-737d-4eef-a577-390ed2d576c9",
   "metadata": {},
   "source": [
    "# 1. Detect document type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8dec0a-09c0-4331-b633-720c90ac12c5",
   "metadata": {},
   "source": [
    "For each input document, specific processing is applied depending on its type, whether it is `PDF`, or `image`.\n",
    "\n",
    "This can be achieved with the helper function `detect_document_type` combined with the `guess` function from the built-in Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5def867e-3789-4496-b08f-2214034650ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_document_type(document_path):\n",
    "    guess_file = guess(document_path)\n",
    "    file_type = \"\"\n",
    "    image_types = [\"jpg\", \"jpeg\", \"png\", \"gif\"]\n",
    "\n",
    "    if guess_file.extension.lower() == \"pdf\":\n",
    "        file_type = \"pdf\"\n",
    "\n",
    "    elif guess_file.extension.lower() in image_types:\n",
    "        file_type = \"image\"\n",
    "\n",
    "    else:\n",
    "        file_type = \"unkown\"\n",
    "\n",
    "    return file_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48729a-715d-40f8-96be-273782f0cd62",
   "metadata": {},
   "source": [
    "Now we can test the function on two types of documents:\n",
    "\n",
    "- `transformer_paper.pdf` is the Transformers research paper [from Arxiv](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "- `zoumana_article_information.png` is the image document containing information about the main topics I have covered on Medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c40d000-a074-47f2-b06b-13b0b7f36c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_path = \"./data/transformer_paper.pdf\"\n",
    "\n",
    "article_information_path = \"./data/zoumana_article_information.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82b65dc-38bb-45d2-b196-128c7121936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper Type: pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Research Paper Type: {detect_document_type(research_paper_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9bb106-496b-4676-9441-01ea20e204ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Information Document Type: image\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Article Information Document Type: {detect_document_type(article_information_path)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55941091-e30a-42b2-8126-0d69cf3c51d4",
   "metadata": {},
   "source": [
    "# 2. Extract content based on document type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5e5a3-40d6-42de-9cd6-41aa4e23559d",
   "metadata": {},
   "source": [
    "The [`langchain`](https://python.langchain.com/docs/get_started/introduction.html) library provides different modules to extract the content of a given type of document.\n",
    "\n",
    "- `UnstructuredImageLoader` extracts image content.\n",
    "- `UnstructuredFileLoader` extracts the content of any pdf and Txt files.  --> didn't work for me\n",
    "\n",
    "We can combine these modules and the above `detect_document_type` function to implement the text extraction logic.\n",
    "\n",
    "These modules can be used to implement end-to-end text extraction logic within the `extract_file_content` function.\n",
    "\n",
    "Let’s see them in action! 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b7660a-4942-4510-87f9-68c283c344ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_content(file_path):\n",
    "    file_type = detect_document_type(file_path)\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        # loader = DirectoryLoader(file_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "        loader = DirectoryLoader(\"./data/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)  # to make it work\n",
    "\n",
    "    elif file_type == \"image\":\n",
    "        loader = UnstructuredImageLoader(file_path)\n",
    "\n",
    "    documents = loader.load()\n",
    "    documents_content = \"\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    return documents_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfb428-f33a-422b-82c7-507961a3e155",
   "metadata": {},
   "source": [
    "Now, let’s print the first 400 characters of each file content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85059d87-41f1-40a3-8457-5a239558d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_content = extract_file_content(research_paper_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a610790a-1501-4a33-89a5-86a798939c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
      "Falling back to partitioning with ocr_only.\n"
     ]
    }
   ],
   "source": [
    "article_information_content = extract_file_content(article_information_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5e3a6-d4a4-46fb-8fb2-dc5534de8e3a",
   "metadata": {},
   "source": [
    "The first 400 characters of each of the above documents are shown below:\n",
    "\n",
    "- The research paper content starts with `Provided proper attribution is provided` and ends with `Jacod Uszkoreit* Google Research usz@google.com`.\n",
    "- The image document’s content starts with `This document provides a quick summary` and ends with `Data Science section covers basic to advance concepts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079ff1e4-5797-4da0-ab19-cb51c4e1f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_characters = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0e4041-55ea-4476-a9c1-f8cdbd404d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 400 Characters of the Paper: \n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Goog ...\n",
      "---------------\n",
      "First 400 Characters of Article Information Document :\n",
      " This document provides a quick summary of some of Zoumana’s article on Medium. It can be considered as the compilation of his 80+ articles about Data Science, Machine Learning and\n",
      "\n",
      "Machine Learning Operations.\n",
      "\n",
      "Whether you are just getting started or you're an experienced professional looking to upskill, these\n",
      "\n",
      "materials can be helpful.\n",
      "\n",
      "Data Science section covers basic to advanced concepts such  ...\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"First {nb_characters} Characters of the Paper: \\n{research_paper_content[:nb_characters]} ...\"\n",
    ")\n",
    "print(\"---\" * 5)\n",
    "print(\n",
    "    f\"First {nb_characters} Characters of Article Information Document :\\n {article_information_content[:nb_characters]} ...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930635b-4b06-4228-86f2-965abf65d327",
   "metadata": {},
   "source": [
    "# 3. Chat Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb785c-b746-4540-9341-24a0478aeaac",
   "metadata": {},
   "source": [
    "The input document is broken into chunks, then an embedding is created for each chunk before implementing the question-answering logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5896df-2c8b-46b7-bdcc-a41fa621bb80",
   "metadata": {},
   "source": [
    "## a. Document chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f39f2-f231-42a9-9a75-d7b151356de0",
   "metadata": {},
   "source": [
    "The chunks represent smaller segments of a larger piece of text. This process is essential to ensure that a piece of content is represented with as little noise as possible, making it semantically relevant.\n",
    "\n",
    "Multiple chunking strategies can be applied. For instance, we have the `NLTKTextSplitter`, `SpacyTextSplitter`, `RecursiveCharacterTextSplitter`, `CharacterTextSplitter` and more.\n",
    "\n",
    "Each one of these strategies has its own pros and cons.\n",
    "\n",
    "The main focus of this article is made on the `CharacterTextSplitter` which creates chunks from the input documents based on `\\n\\n`, and measure each chunk's length (`length_function`) by its number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b182e5-4be0-4e0f-89bd-abc2812b167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab08377-85aa-4d71-b95a-d195c9363d6c",
   "metadata": {},
   "source": [
    "The `chunk_size` tells that we want a maximum of 1000 characters in each chunk, and a smaller value will result in more chunks, while a larger one will generate fewer chunks.\n",
    "\n",
    "It is important to note that the way the `chunk_size` is chosen can affect the overall result. So, a good approach is to try different values and chose the one that better fits one's use case.\n",
    "\n",
    "Also, the `chunk_overlap` means that we want a maximum of 200 overlapping characters between consecutive chunks.\n",
    "\n",
    "For instance, imagine that we have a document containing the text `Chat with your documents using LLMs` and want to apply the chunking using the `Chunk Size = 10` and `Chunk overlap = 5`.\n",
    "\n",
    "The process is explained in the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428e993-74a0-47af-ba5b-d95087c76af2",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure_2.webp\" style=\"width:700px;height:500px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3862b5a-0027-44f8-8b6e-cf998d27f482",
   "metadata": {},
   "source": [
    "We can see that we ended up with a total of 7 chunks for an input document of 35 characters (spaces included).\n",
    "\n",
    "But, why do we use these overlaps in the first place?\n",
    "\n",
    "Including these overlaps, the `CharacterTextSplitter` ensures that the underlying context is maintained between the chunks, which is especially useful when working with long pieces of documents.\n",
    "\n",
    "Similarly to the `chunk_size` there is no fixed value of `chunk_overlap`. Different values need to be tested to choose the one with better results.\n",
    "\n",
    "Now, let’s see their application in our scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff0f336-116f-471b-8b25-32f848cc55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_chunks = text_splitter.split_text(research_paper_content)\n",
    "article_information_chunks = text_splitter.split_text(article_information_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccc5f71-292e-4b4f-9021-46a95d007a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chunks in Research Paper: 1\n",
      "# Chunks in Article Document: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"# Chunks in Research Paper: {len(research_paper_chunks)}\")\n",
    "print(f\"# Chunks in Article Document: {len(article_information_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d155d62-dd95-495d-af5c-1959b526fd80",
   "metadata": {},
   "source": [
    "For a larger document like the research paper, we have a lot more chunks (51) compared to the one-page article document, which is only 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9415310-b524-4055-82a6-8dad26477240",
   "metadata": {},
   "source": [
    "## b. Create embeddings of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c5989-1cd7-4774-9d2f-74dc3a3f2256",
   "metadata": {},
   "source": [
    "We can use the `OpenAIEmbeddings` module, which uses `text-embedding-ada-002` model by default to create the embedding of the chunks.\n",
    "\n",
    "Instead of using the `text-embedding-ada-002` can use a different model (e.g. `gpt-3.5-turbo-0301`) by changing the following parameters:\n",
    "\n",
    "- model = “`gpt-3.5-turbo-0301`”\n",
    "- deployment = \"`<DEPLOYMENT-NAME>`“ which corresponds to the name given during the deployment of the model. The default value is also `text-embedding-ada-002`\n",
    "\n",
    "For simplicity’s sake, we will stick to using the default parameters’ value in this tutorial. But before that, we need to acquire the OpenAI credentials, and all the steps are provided in the [following article](https://medium.com/geekculture/how-to-fine-tune-gpt3-using-openai-api-and-python-9ef813879af4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5368dcb7-4d10-4935-afb2-57158ae3e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"Your Key\"\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b509dc4-cc92-4fed-b857-5e7eb24c6e58",
   "metadata": {},
   "source": [
    "## c. Create document search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f90676-fd9d-4408-bd9b-a10396a8d3a5",
   "metadata": {},
   "source": [
    "To get the answer to a given query, we need to create a vector store that finds the closest matching chunk to that query.\n",
    "\n",
    "Such vector store can be created using the `from_texts` function from `FAISS` module and the function takes two main parameters: `text_splitter` and `embeddings` which are both defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c12bac-3441-4dcc-b070-4722bbfa4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_search(text_splitter):\n",
    "    return FAISS.from_texts(text_splitter, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3573448-b1cd-46bb-938b-cfb8984cfdb9",
   "metadata": {},
   "source": [
    "By running the `get_doc_search` on the research paper chunks, we can see that the result is of a `vectorstores`. The result would have been the same if we used the article_information_chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f64c1e-326e-4767-b698-d39203575552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc_search_paper \u001b[38;5;241m=\u001b[39m \u001b[43mget_doc_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_paper_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m, in \u001b[0;36mget_doc_search\u001b[1;34m(text_splitter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_doc_search\u001b[39m(text_splitter):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:384\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    366\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    386\u001b[0m         texts,\n\u001b[0;32m    387\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    391\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\langchain\\embeddings\\openai.py:234\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# handle batches of large input text\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\langchain\\embeddings\\openai.py:175\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    173\u001b[0m _chunk_size \u001b[38;5;241m=\u001b[39m chunk_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), _chunk_size):\n\u001b[1;32m--> 175\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     batched_embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    182\u001b[0m results: List[List[List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\langchain\\embeddings\\openai.py:63\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _embed_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\langchain\\embeddings\\openai.py:61\u001b[0m, in \u001b[0;36membed_with_retry.<locals>._embed_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpt4all\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "doc_search_paper = get_doc_search(research_paper_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884dc71-bf1c-4811-8533-cd2280fa49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239a7e5-92d9-4ba2-b4c0-d275d90dc850",
   "metadata": {},
   "source": [
    "## d. Start chatting with your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e62d1-c811-4d8e-965e-09f8206dbd9e",
   "metadata": {},
   "source": [
    "The `chat_with_file` function is used to implement the end-to-end logic of the chat by combining all the above functions, along with the with `similarity_search` function.\n",
    "\n",
    "The final function takes two parameters:\n",
    "\n",
    "- The file we want to chat with, and\n",
    "- The query provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851597c9-4412-4a3a-becd-3f4c8567e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"map_rerank\", return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2219bd-55ac-4fd4-9bf2-c6115575eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_file(file_path, query):\n",
    "    file_content = extract_file_content(file_path)\n",
    "    text_splitter = text_splitter.split_text(file_content)\n",
    "\n",
    "    document_search = get_doc_search(text_splitter)\n",
    "    documents = document_search.similarity_search(query)\n",
    "\n",
    "    results = chain(\n",
    "        {\"input_documents\": documents, \"question\": query}, return_only_outputs=True\n",
    "    )\n",
    "    answers = results[\"intermediate_steps\"][0]\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e320bf4-8b34-4853-9821-a6f3eebe1d85",
   "metadata": {},
   "source": [
    "Let’s take a step back to properly understand what is happening in the above code block.\n",
    "\n",
    "- The `load_qa_chain` provides an interface to perform question-answering over a set of documents. In this specific case, we are using the default `OpenAI GPT-3` large language model.\n",
    "- The `chain_type` is `map_rerank`. By doing so, the `load_qa_chain` function returns the answers based on a confidence score given by the chain. There are other `chain_type` that can be used such as `map_reduce`, `stuff`, `refine` and more. Each one has its own pros and cons.\n",
    "- By setting `return_intermediate_steps=True`, we can access the metadata such as the above confidence score.\n",
    "\n",
    "Its output is a dictionary of two keys: the **answer** to the query, and the **confidence score**.\n",
    "\n",
    "We can finally chat with the our files, starting with the image document:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07afb3-5491-41c5-bd11-8bd4d0295139",
   "metadata": {},
   "source": [
    "### d-1. Chat with the image document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0723ab-832e-464f-9f4a-43f82e11b1a0",
   "metadata": {},
   "source": [
    "To chat with the image document, we provide the path to the document, and the question we want the model to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c8384-4da0-444d-9a8d-c506bd410cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the document about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38281b7-b463-43fe-a493-0c60dada4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chat_with_file(article_information_path, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdc14b-e490-456b-9d0e-3332545df132",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = results[\"answer\"]\n",
    "confidence_score = results[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be903e9-c84d-4d40-8fdc-13226c6cddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Answer: {answer}\\n\\nConfidence Score: {confidence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb1ad2-bdff-4965-ade5-bc596b80f265",
   "metadata": {},
   "source": [
    "The model is 100% confident in its response. By looking at the first paragraph of the original document below, we can see that the model response is indeed correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c27746-92cd-4d14-b04c-ba30d421f7ac",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure_3.webp\" style=\"width:700px;height:120px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03039e2-69a6-4556-99fe-0e3899504833",
   "metadata": {},
   "source": [
    "One of the most interesting parts is that it provided a brief summary of the main topics covered in the document ( statistics, model evaluation metrics, SQL queries, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fa0d2-7ace-4942-9972-52323a7ec9d4",
   "metadata": {},
   "source": [
    "### d-2. Chat with the PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcf49c-26d4-4c38-84a2-483b79e81785",
   "metadata": {},
   "source": [
    "The process with the PDF file is similar to the one in the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeb595-d72d-4086-b96f-7aefcd7f5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is the self-attention approach used in this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fc60d-4202-4d67-b582-8fcee6a05aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chat_with_file(research_paper_path, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1a1db-2a11-410b-ae62-af2b25092d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = results[\"answer\"]\n",
    "confidence_score = results[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89f1d9-372f-48db-a4c3-9fecc7bc1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Answer: {answer}\\n\\nConfidence Score: {confidence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8a3aa-1f33-4002-8484-fb1c955173bb",
   "metadata": {},
   "source": [
    "Once again we are getting a 100% confidence score from the model. The answer to the question looks pretty correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa597a-5692-4454-bb2c-6999e9805800",
   "metadata": {},
   "source": [
    "In both cases, the model was able to provide a human-like response in a few seconds. Making a human go through the same process would take minutes, even hours depending on the length of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccb627-1b1f-42c3-81ee-670b338c5eca",
   "metadata": {},
   "source": [
    "# MAKE SURE TO REMOVE OPENAI KEY BEFORE PUSSHING TO GITHUB!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c490e99-13d7-4563-99c1-d78c9ce5b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpt4all]",
   "language": "python",
   "name": "conda-env-gpt4all-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
