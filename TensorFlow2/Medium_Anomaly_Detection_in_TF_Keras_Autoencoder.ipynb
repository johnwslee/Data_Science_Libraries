{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8583782-4f5a-4131-8e3e-737382b71bdc",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/towards-data-science/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50) in Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08e2c1-1dc4-462a-a414-817ea56a5172",
   "metadata": {},
   "source": [
    "In this tutorial, I will explain in detail how an autoencoder works with a working example.\n",
    "\n",
    "For this example, I chose to use a [public dataset](https://github.com/AlexOlsen/DeepWeeds) (Apache License 2.0) named deep_weeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8f6b41-a1e8-4213-89c1-36b394152987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068e747b-266c-4401-a053-99434f118410",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\"deep_weeds\", split=\"train\", shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824d931e-301c-4819-b33a-69b3de5422ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'image': TensorSpec(shape=(256, 256, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa739b4-43c7-475c-be13-61cb073105db",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100fc8a9-f0b2-42da-84ac-e0db33a50727",
   "metadata": {},
   "source": [
    "We need to prepare a dataset for this unsupervised anomaly detection example. Only one class will be taken as our main class that will be considered as the valid class. And I will put a few data from another class as an anomaly. Then we will develop the model to see if we can find that few anomaly data.\n",
    "\n",
    "I chose class 5 as the valid class and class 1 as the anomaly. In the code block below, I am taking all the data of classes 5 and 1 first and creating lists of the images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30fb4735-c31a-4fed-855c-cf2342a7acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a41bc8-6883-4804-a2a8-f09c42cfae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_main = []\n",
    "images_anomaly = []\n",
    "labels_main = []\n",
    "labels_anomaly = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d491e56-c2ad-4983-9a48-194f20735f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b00bfbe-3a5b-4b15-a11c-7420d8bd04fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'image': TensorSpec(shape=(256, 256, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45499c0d-d55d-49de-9325-ddfe3accab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in ds:\n",
    "    # print(np.array(example['label']))\n",
    "    if np.array(example[\"label\"]) == 5:\n",
    "        images_main.append(example[\"image\"])\n",
    "        labels_main.append(example[\"label\"])\n",
    "    if np.array(example[\"label\"]) == 1:\n",
    "        images_anomaly.append(example[\"image\"])\n",
    "        labels_anomaly.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d90bf8-d750-41a0-ad55-c656ec49f471",
   "metadata": {},
   "source": [
    "Let’s see the shape of the main image (images of class 5) data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac5e1ac-1f85-445d-9d83-5dcf9a6e8aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1009, 256, 256, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(images_main).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4af382-895e-4319-a362-6602a4fc049c",
   "metadata": {},
   "source": [
    "The image shapes are (256, 256, 3) and we have a total of 1009 data for class 5.\n",
    "\n",
    "However, we do not need all the data from class 1. Because class 1 is the anomaly class. So, only 1% of the class 1 data will be taken for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3874a894-ac74-4e6a-9bf9-576b25ecb18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parc = round(len(labels_anomaly) * 0.01)\n",
    "images_anomaly = np.array(images_anomaly)[:parc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969fc0a4-6c0a-4d90-8074-63ebfd7a295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the main images and anomaly images together\n",
    "total_images = np.vstack([images_main, images_anomaly])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af136e-4090-4018-a2bd-903cf09f56b9",
   "metadata": {},
   "source": [
    "The shape of the total_images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31eb4bda-96d9-4282-930d-688a2d6ee96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 256, 256, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841a57e-d876-44fd-972f-516e9352e9f4",
   "metadata": {},
   "source": [
    "We have a total of 1020 images for training. As we saw earlier, we have 1009 class 5 images, and we took 1020–1009 = 11 of class 1 images which is our anomaly.\n",
    "\n",
    "Let’s see if we can develop an autoencoder model in Keras and Tensorflow to detect these anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbb58f-9d8f-4cc2-9987-76405e5c416a",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a486042d-9115-4a88-91d6-46d504da5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    Reshape,\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c834e5-5226-448a-9009-5e2bbe7e0c59",
   "metadata": {},
   "source": [
    "Some of the data should be kept separately for testing purposes. The train_test_split method from the sklearn library can be used for that. Remember, as this is an unsupervised learning method, the labels are not necessary. We will only split the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ef4406-a479-41ed-a08c-ef1432c7113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = train_test_split(total_images, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdcd41-c48c-4b4c-9c84-eb7b5d4deccf",
   "metadata": {},
   "source": [
    "Finally, the autoencoder model. We will build a `Convolution_Autoencoder` class which is a Convolutional Neural Network. The class has the build method where we will define the Autoencoder model.\n",
    "\n",
    "The ‘build’ takes `width`, `depth`, `height`, `filters`, and `latentDim` as parameters. Here, width, depth, and height are the dimensions of the images that is (256, 256, 3) for us as we have seen with the `total_images.shape` method above.\n",
    "\n",
    "The parameter `filters` is the filter for the convolution layers.\n",
    "\n",
    "The `latentDim` is the size of our compressed layer after the encoder method.\n",
    "\n",
    "In this build method, the first part is an encoder model which is a simple Convolutional Neural Network.\n",
    "\n",
    "Once the encoder portion is done, a decoder model is developed using `Conv2DTranspose` layers to reconstruct the data again.\n",
    "\n",
    "Then, we construct the autoencoder model which is actually a combination of both encoder and decoder models.\n",
    "\n",
    "Finally, we return the encoder, decoder, and autoencoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da896f63-e6e8-497b-8230-ddc99d686fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution_Autoencoder:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, filters=(16, 32, 64), latentDim=32):\n",
    "        input_shape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "\n",
    "        for f in filters:\n",
    "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        volume = K.int_shape(x)\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latentDim)(x)\n",
    "\n",
    "        # encoder model\n",
    "        encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "        # compressed representation\n",
    "        latent_layer_input = Input(shape=(latentDim,))\n",
    "        x = Dense(np.prod(volume[1:]))(latent_layer_input)\n",
    "\n",
    "        x = Reshape((volume[1], volume[2], volume[3]))(x)\n",
    "\n",
    "        # Recostructing the image with a decoder model\n",
    "        for f in filters[::-1]:\n",
    "            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        decoder = Model(latent_layer_input, outputs, name=\"decoder\")\n",
    "\n",
    "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "\n",
    "        return (encoder, decoder, autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e34d89-e4c9-4454-95c0-e378b92bbfa8",
   "metadata": {},
   "source": [
    "Model development is done. It’s time to run the model and see if it works. It should run like any other TensorFlow model.\n",
    "\n",
    "Here we will compile the model first with Adam optimizer. And also, I used a decay in the learning rate and the ‘mse’ as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763a3607-0897-4ae9-9c0c-df2661ac91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr_start = 0.001\n",
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d21e143d-53c3-43c5-be6d-243f39cd03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder, decoder, autoencoder) = Convolution_Autoencoder.build(256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02319869-3e95-4a43-acdd-1817dd94b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr_start, decay=lr_start / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d60da27-0719-4c57-ac86-0c461aee9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss=\"mse\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5012c3-4705-472a-b7e0-c4cd2c06e876",
   "metadata": {},
   "source": [
    "Finally, running the model. Remember, this is an unsupervised learning method. So there won't be any label in the model training. Instead, we need to pass two training features which will be just train_x twice. If you notice the build method in the Convolution_Autoencoder class, autoencoder looks like this there:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2f9457-eb14-4c36-b779-a0760bd66a88",
   "metadata": {},
   "source": [
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e53f0-bc9f-43a4-8896-81039cf51d1b",
   "metadata": {},
   "source": [
    "In the Model above, we need to pass inputs which is train_x first, and then decoder(encoder(inputs)) where we need to pass the train_x again. Same for the test_x as well.\n",
    "\n",
    "Before you begin the model training, I should warn you that it is very slow in the default setting of Google Colab. You can make it way faster by running this in the GPU. Please change the settings of your Google Colab notebook before you run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377a095b-91a7-43f0-a164-6f5781efaaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 40s 1s/step - loss: 13060.3281 - val_loss: 13119.6787\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 13018.2012 - val_loss: 13057.1123\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 13002.2949 - val_loss: 13002.3154\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12996.8301 - val_loss: 13033.1592\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12993.4902 - val_loss: 12995.9346\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12990.6055 - val_loss: 12971.4199\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12989.3564 - val_loss: 12971.3848\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 39s 2s/step - loss: 12987.3311 - val_loss: 12968.4521\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12990.6260 - val_loss: 13002.3018\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12987.5195 - val_loss: 12979.4023\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12986.5049 - val_loss: 12974.4766\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12985.7295 - val_loss: 12971.1123\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12985.1504 - val_loss: 12968.2588\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12984.8975 - val_loss: 12965.4375\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 39s 2s/step - loss: 12984.3662 - val_loss: 12965.1221\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12983.8154 - val_loss: 12965.4531\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12983.5576 - val_loss: 12963.5430\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12983.4004 - val_loss: 12964.2178\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12983.0156 - val_loss: 12963.6543\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12982.7451 - val_loss: 12963.4551\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12982.5244 - val_loss: 12963.1836\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12982.4902 - val_loss: 12963.1914\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12982.1963 - val_loss: 12963.1787\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12982.1621 - val_loss: 12967.7061\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12982.0186 - val_loss: 12964.3613\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12981.9053 - val_loss: 12963.1641\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12981.8828 - val_loss: 12963.1680\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 38s 1s/step - loss: 12981.7881 - val_loss: 12963.1768\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12981.7451 - val_loss: 12963.0029\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12981.7080 - val_loss: 12962.9863\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(\n",
    "    train_x, train_x, validation_data=(test_x, test_x), epochs=30, batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d621af7-0c6b-41dc-8219-ebd8be11846b",
   "metadata": {},
   "source": [
    "As you can see there are not many changes to losses, simply because here we do not have labels. Instead, we pass the training features to it twice. Losses come from comparing the original images to the reconstructed images by autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b87fa-a401-4a00-a3af-e0890173af0b",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d822b-bf8c-488e-a5d3-4e22a54d8615",
   "metadata": {},
   "source": [
    "Model evaluation is different from a regular supervised learning model in autoencoders as this is not a supervised learning method. Let’s do that step by step.\n",
    "\n",
    "First, we will do the prediction as usual, which will be the decoded images by the autoencoder model.\n",
    "\n",
    "Then, you calculate the mean squared error using the original errors and the reconstructed error and save it to the ‘errors’ list. Here is the code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584fb0be-c2f5-41a4-86bd-5f72212aa0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 393ms/step\n"
     ]
    }
   ],
   "source": [
    "decoded = autoencoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b139b1b5-9645-4a88-b5e3-1e2ec88db1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for image, recon in zip(total_images, decoded):\n",
    "    mse = np.mean((image - recon) ** 2)\n",
    "    errors.append(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a6aad-ec2d-4709-9f22-c1d78a81614e",
   "metadata": {},
   "source": [
    "As we have the ‘mse’ for all the images in the test set, we choose a threshold. Here I am using 95% quantile using np. quantile method and getting indices from the ‘errors’ where ‘mse’ is greater than the threshold. When ‘mse’ is greater than the threshold error we decided we will consider them as an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d99bd403-748d-4522-a059-baedc86d5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.quantile(errors, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1b1e386-e62a-4df1-adc2-96fe4a5bc942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   9,  35,  55,  88, 135, 145, 184, 196, 201, 202], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.where(np.array(errors) >= threshold)[0]\n",
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11216b7f-3655-434b-99e8-080ae54e5d46",
   "metadata": {},
   "source": [
    "Now, let’s get back to the image dataset ‘total_images’ that we prepared for the training earlier. We need to check if the indices we have which are more than the threshold are actually the anomaly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df3c421-3a9d-429f-84d5-9ece46bdfed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in idxs:\n",
    "    if total_images[i] in images_anomaly:\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6e17c-5b4d-41d0-9b12-4bbf8cf22c71",
   "metadata": {},
   "source": [
    "Yes!! They are all anomaly data. If you count the number of ‘True’ above we have 11 ‘True’ here. We can check how many anomaly data we originally had in the ‘images_anomaly’:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7cad6e5-fd2d-455f-bab7-14404191cfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddb7eb-01c6-46ae-b90d-fa68a1f3781b",
   "metadata": {},
   "source": [
    "So, we found all the anomaly data using the autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841c2f5-f7ef-4e92-82f6-1b7e1b11317c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
