{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79031df7-26de-4a05-8957-984c1adf06a6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749694c7-3483-45ee-b89d-cb3a7e48dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, CenterCrop\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import PIL\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14eebe5-2a41-483d-b411-3cc28b70f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9e92e-620f-49e3-a18d-c5ed321b9532",
   "metadata": {},
   "source": [
    "# Preparation of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f17b6b-c5dd-4dcb-96f7-49665cedeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../fiftyone/coco-2014/info.json\", 'r') as file:\n",
    "    info = json.load(file)\n",
    "\n",
    "classes = tuple(info['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae0c8b-6487-4210-92a9-b82316380d84",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cac3325-76ef-4cd5-8610-c55a03e51e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('vgg16_multi-label_coco-2014_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b52823f-66f1-464e-9c2a-2a29a3f13008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('vgg16_multi-label_coco-2014_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c8c724-60ef-4082-bdfb-f7eab405a13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.4, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=91, bias=True)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4e0088-b306-4b31-a766-b5cfe998f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1618c58-3854-4ebd-81a9-0fce0cd63448",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b040fb-ed60-4c7e-819a-f0524609de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    img = Resize(size=256)(img)\n",
    "    img = CenterCrop(size=224)(img)  # 224 x 224 is required for vgg16\n",
    "    img_array = ToTensor()(img)\n",
    "    img_array = Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])(img_array)\n",
    "    img_array = img_array.unsqueeze(0)\n",
    "    prediction = model(img_array)\n",
    "    predictions_label = prediction > 0.5\n",
    "    # prediction = nn.Softmax(dim=1)(prediction)\n",
    "    # prediction = torch.topk(prediction, 10)   <- don't need this because Gradio takes care of this\n",
    "    predicted_labels = [classes[label] for label in get_index_positions(list(predictions_label[0].cpu().numpy()), 1)]\n",
    "    # confidences = {classes[i]: float(prediction[0][i].detach()) for i in range(len(classes))}\n",
    "    confidences = \", \".join(predicted_labels)\n",
    "    if confidences == \"\":\n",
    "        confidences = {classes[i]: float(prediction[0][i].detach()) for i in range(len(classes))}\n",
    "    else:\n",
    "        confidences = confidences\n",
    "    \n",
    "    return confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87accf5b-8f51-4dc0-ba7e-42541ee42001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x22d0d8f3b20>, 'http://127.0.0.1:7864/', None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(shape=(256, 256), type='pil'),\n",
    "    # outputs=gr.Label()\n",
    "    outputs=gr.Label(num_top_classes=3)\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a5e80-dc1b-4b5c-8a80-c3c18ad74d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
