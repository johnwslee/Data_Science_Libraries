{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfe72e6-fb75-49b7-9150-58ac8e938628",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/towards-data-science/object-oriented-data-science-refactoring-code-5bcb4ae7ce72) in Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52cffa4-3bd9-4681-bee0-28a7eaf32988",
   "metadata": {},
   "source": [
    "For data scientists, code is the backbone of analysis and decision-making. As data science applications grow more intricate, from machine learning models embedded in software to complex data pipelines orchestrating vast amounts of information, developing clean, organized, and maintainable code becomes crucial. Object-oriented programming (OOP) unlocks flexibility and efficiencies that enable data scientists to respond to changing requirements with agility. OOP introduces the concept of classes, which serve as blueprints for creating objects that encapsulate both data and the operations that manipulate it. This paradigm shift allows data scientists to go beyond traditional functional approaches, promoting modular design and code reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa145f-a273-4841-8b3b-26a9d6870abc",
   "metadata": {},
   "source": [
    "In this article, we’ll explore the benefits of refactoring data science code by creating classes and deploying object-oriented techniques, and how this approach can enhance modularity and reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3d3b0-f1d3-4e7f-a469-20e4a6a8653d",
   "metadata": {},
   "source": [
    "# The Power of Classes in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c468e-b1b1-4b6f-abb0-7bb5407c45c7",
   "metadata": {},
   "source": [
    "In traditional data science workflows, functions have been the approach for encapsulating logic. This is often sufficient as functions allow developers to minimize repeated code. However, as projects evolve, maintaining an extensive collection of functions might lead to code that’s challenging to navigate, debug, and scale.\n",
    "\n",
    "This is where classes come into play. A class is a blueprint for creating objects, which bundle both data and functions (called methods) that operate on that data. By organizing code into classes, developers can achieve several advantages:\n",
    "\n",
    "1. Modularity and Encapsulation: Classes promote modularity by grouping related functionality together. Each class encapsulates its own attributes (data) and methods (functions), reducing the risk of global variable pollution and the potential for naming conflicts. This helps maintain a clear separation of concerns, making code easier to understand and modify.\n",
    "2. Reusability: Classes encourage reusability by providing a consistent interface for similar tasks across different parts of the project. Once a class is defined, it can be instantiated whenever needed and its methods can be used to achieve consistent results.\n",
    "3. Inheritance and Polymorphism: Inheritance allows developers to create subclasses that inherit attributes and methods from a parent class. This promotes code reuse while enabling customization for specific tasks. Polymorphism, another OOP concept, lets developers use the same method name across different classes, adapting behavior based on the specific implementation.\n",
    "4. Testing and Debugging: Classes facilitate unit testing, as test cases can target individual methods within a class, making it easier to identify and fix issues, improving the overall robustness of your codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81153c-9443-4610-a71d-2fc8d6a8175c",
   "metadata": {},
   "source": [
    "# Refactoring to Classes: A Theoretical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b04851-910f-413b-9a1f-8ed59df94ab8",
   "metadata": {},
   "source": [
    "Let’s consider a scenario where you’re working on a machine learning project that involves data preprocessing, model training, and evaluation. Initially, you might have a collection of functions for each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c9579c-23c6-4417-8470-ec89c84989a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using functions for data preprocessing\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load and preprocess data\n",
    "    ...\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Clean, transform, and encode data\n",
    "    ...\n",
    "\n",
    "\n",
    "def train_model(preprocessed_data):\n",
    "    # Train a machine learning model\n",
    "    ...\n",
    "\n",
    "\n",
    "def evaluate_model(trained_model, test_data):\n",
    "    # Evaluate model performance\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f2c0d-4a40-47ed-82a2-bd62e51eefd9",
   "metadata": {},
   "source": [
    "While functional decomposition works, over time, there may be many steps that occur within the preprocessing, training, and evaluation. This can become challenging to manage these functions.\n",
    "\n",
    "Refactoring the code into classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb8b7a8-d30f-468e-b8bf-373f5831248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.data = self.load_data(file_path)\n",
    "        self.preprocessed_data = self.preprocess_data()\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        # Load and preprocess data\n",
    "        ...\n",
    "\n",
    "    def clean_data(self):\n",
    "        # imputation, outlier treatment\n",
    "        ...\n",
    "\n",
    "    def transform_data(self):\n",
    "        # transformations and encode data\n",
    "        ...\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, preprocessed_data):\n",
    "        self.model = self.train_model(preprocessed_data)\n",
    "\n",
    "    def fit(self, preprocessed_data):\n",
    "        # Train a machine learning model\n",
    "        ...\n",
    "\n",
    "    def predict(self, preprocessed_data):\n",
    "        # Predict using the machine learning model\n",
    "        ...\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, trained_model, test_data):\n",
    "        self.performance_metrics = self.evaluate_model(trained_model, test_data)\n",
    "\n",
    "    def evaluate_model(self, trained_model, test_data):\n",
    "        # Evaluate model performance\n",
    "        ...\n",
    "\n",
    "    def calculate_rmse(self, trained_model, test_data):\n",
    "        # Evaluate root mean squared error\n",
    "        ...\n",
    "\n",
    "    def calculate_r_squared(self, trained_model, test_data):\n",
    "        # Evaluate r_squared of the model\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039fe4f-ead9-408c-83e5-16748a22fe63",
   "metadata": {},
   "source": [
    "By breaking down the workflow into classes, there is more organization, and the structure is easier to read and maintain. Each class handles a specific aspect of the process. They can be instantiated as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "671eb0ec-56b6-4115-a242-9f69639208ba",
   "metadata": {},
   "source": [
    "data_preprocessor = DataPreprocessor('data.csv')\n",
    "model_trainer = ModelTrainer(data_preprocessor.preprocessed_data)\n",
    "model_evaluator = ModelEvaluator(model_trainer.model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771a991-a534-4804-9d28-a781c4ea206c",
   "metadata": {},
   "source": [
    "In this case, the classes have incorporating classes provides an extra layer of structure and flexibility that improve the workflow and usability of the code. By embracing the power of classes, this example creates a more robust and scalable code base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9719167-040a-49a1-9573-7d226ce61fc0",
   "metadata": {},
   "source": [
    "# Refactoring to Classes: A Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22427928-2d4b-4b21-9c16-66f619622528",
   "metadata": {},
   "source": [
    "As a practical example, I recently refactored code that was developed in [this repository](https://github.com/mollyryanruby/sales_forecasting) 3 years ago into [a new repository](https://github.com/mollyryanruby/auto_forecast) to show the difference in the code before and after refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bbf82-0e83-4c78-b54e-ffad436f97ee",
   "metadata": {},
   "source": [
    "In the initial repository, many functions encompassed modeling tasks as several different models were trained and tested. In the refactored version, there is a model class SalesForecasting that encompasses all the modeling tasks. This is easier to read and allows the package to be deployed more efficiently as SalesForecasting and be instantiated multiple times with different inputs. As a preview the class looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cc9bca-32f3-4b01-a7b4-7f49abf1f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesForecasting:\n",
    "    \"\"\"\n",
    "    SalesForecasting class to train and predict sales using a variety of models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_list):\n",
    "        \"\"\"\n",
    "        Initialize the SalesForecasting class with a list of models to train and predict.\n",
    "\n",
    "        Args:\n",
    "            model_list (list): list of models to train and predict. Options include:\n",
    "                - LinearRegression\n",
    "                - RandomForest\n",
    "                - XGBoost\n",
    "                - LSTM\n",
    "                - ARIMA\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the models in model_dict to the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): training data exogonous features for the model\n",
    "            y_train (pd.Series): training data target for the model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def __fit_regression_model(self, model):\n",
    "        \"\"\"\n",
    "        Fit a regression model to the training data.\n",
    "\n",
    "        Args:\n",
    "            model (sklearn model): sklearn model to fit to the training data\n",
    "\n",
    "        Returns:\n",
    "            model (sklearn model): fitted sklearn model\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __fit_lstm_model(self, model):\n",
    "        \"\"\"\n",
    "        Fit an LSTM model to the training data.\n",
    "\n",
    "        Args:\n",
    "            model (keras model): keras model to fit to the training data\n",
    "\n",
    "        Returns:\n",
    "            model (keras model): fitted keras model\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def __fit_arima_model(self, model_name):\n",
    "        \"\"\"\n",
    "        Fit an ARIMA model to the training data.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): name of the model to fit to the training data\n",
    "\n",
    "        Returns:\n",
    "            model (pmdarima model): fitted pmdarima model\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict(self, x_values, y_values=None, scaler=None, print_scores=False):\n",
    "        \"\"\"\n",
    "        Predict values using the models in model_dict.\n",
    "\n",
    "        Args:\n",
    "            x_values (pd.DataFrame): exogenous features to predict on\n",
    "            y_values (pd.Series): target values to compare predictions against\n",
    "            scaler (sklearn scaler): scaler used to scale the data\n",
    "            print_scores (bool): whether to print the scores for each model\n",
    "\n",
    "        Returns:\n",
    "            self (SalesForecasting): self with updated predictions\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def __predict_regression_model(self, model):\n",
    "        \"\"\"\n",
    "        Predict values using a regression model.\n",
    "\n",
    "        Args:\n",
    "            model (sklearn model): sklearn model to predict with\n",
    "\n",
    "        Returns:\n",
    "            predictions (np.array): array of predictions\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __predict_lstm_model(self, model):\n",
    "        \"\"\"\n",
    "        Predict values using an LSTM model.\n",
    "\n",
    "        Args:\n",
    "            model (keras model): keras model to predict with\n",
    "\n",
    "        Returns:\n",
    "            predictions (np.array): array of predictions\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __predict_arima_model(self, model):\n",
    "        \"\"\"\n",
    "        Predict values using an ARIMA model.\n",
    "\n",
    "        Args:\n",
    "            model (pmdarima model): pmdarima model to predict with\n",
    "        Returns:\n",
    "            predictions (np.array): array of predictions\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __undo_scaling(self, values, scaler):\n",
    "        \"\"\"\n",
    "        Undo scaling on a set of values.\n",
    "\n",
    "        Args:\n",
    "            values (np.array): array of values to unscale\n",
    "            scaler (sklearn scaler): scaler to use to unscale the values\n",
    "\n",
    "        Returns:\n",
    "            unscaled_values (np.array): array of unscaled values\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def get_scores(self, y_pred, y_true, model_name=None, print_scores=False):\n",
    "        \"\"\"\n",
    "        Get the scores for a model. Scores include RMSE, MAE, and R2.\n",
    "\n",
    "        Args:\n",
    "            y_pred (np.array): array of predicted values\n",
    "            y_true (np.array): array of true values\n",
    "            model_name (str): name of the model to get scores for\n",
    "            print_scores (bool): whether to print the scores for the model\n",
    "\n",
    "        Returns:\n",
    "            rmse (float): root mean squared error\n",
    "            mae (float): mean absolute error\n",
    "            r2 (float): r squared\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def plot_results(\n",
    "        self,\n",
    "        model_list=None,\n",
    "        figsize=(13, 3),\n",
    "        xlabel=\"Date\",\n",
    "        ylabel=\"Sales\",\n",
    "        title=\"Sales Forecasting Predictions\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot the results of the predictions against the actual values.\n",
    "        Generates a timeseries for predictions from each model in model_dict.\n",
    "\n",
    "        Args:\n",
    "            model_list (list): list of models to plot. If None, plots all models in model_dict\n",
    "            figsize (tuple): tuple of figure size\n",
    "            xlabel (str): label for x axis\n",
    "            ylabel (str): label for y axis\n",
    "            title (str): title for the plot\n",
    "\n",
    "        Returns:\n",
    "            fig (matplotlib figure): figure with the plot\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def plot_errs(self, figsize=(13, 3)):\n",
    "        \"\"\"\n",
    "        Plot the errors for each model in model_dict. Errors include RMSE, MAE, and R2.\n",
    "\n",
    "        Args:\n",
    "            figsize (tuple): tuple of figure size\n",
    "\n",
    "        Returns:\n",
    "            fig (matplotlib figure): figure with the plot\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5516b-ac9b-4247-9c1c-e8caee9e7109",
   "metadata": {},
   "source": [
    "The class “SalesForecasting” serves as a comprehensive blueprint for data-driven businesses to anticipate future sales trends through the application of various predictive models. Within this class, data scientists can harness the power of different modeling techniques, including Linear Regression, Random Forest, XGBoost, LSTM (Long Short-Term Memory), and ARIMA (AutoRegressive Integrated Moving Average). By encapsulating the forecasting workflow within this class, the process of model fitting, prediction, and evaluation becomes streamlined and consistent across different model types. Through the “SalesForecasting” class, data scientists can efficiently experiment with different algorithms and easily maintain the code base.\n",
    "\n",
    "Object-oriented programming is a tool for data scientists to architect code that mirrors the intricacies of the real-world systems they analyze, enabling them to extract valuable insights while maximizing agility. Although python intends for classes to be used for instantiation and inheritance, the example above shows a first step in which classes are leveraged for modularizing code. As data science capabilities expand and teams grow, maintaining efficient code is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fd67b-c005-42b9-aebf-1002a752742c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ls_env]",
   "language": "python",
   "name": "conda-env-ls_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
