{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f825f337-1e94-4ece-8d06-49f23115a59e",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161) in Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe617bec-9f3f-4734-9c05-f0fdba5f4316",
   "metadata": {},
   "source": [
    "# 1. What is Hugging Face?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c87f32-67a7-44a7-9648-ab75b1c02daa",
   "metadata": {},
   "source": [
    "Hugging Face is an AI company that has become a major hub for open-source machine learning (ML). Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
    "\n",
    "First is their rapidly growing repository of pre-trained open-source ML models for things such as natural language processing (NLP), computer vision, and more. Second is their library of datasets for training ML models for almost any task. Third, and finally, is Spaces which is a collection of open-source ML apps hosted by Hugging Face.\n",
    "\n",
    "The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem ‚Äî the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013634-b373-42ae-b74a-5a6f67500b83",
   "metadata": {},
   "source": [
    "# 2. ü§óTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616a296-fe8e-4556-862a-7865ac0ace04",
   "metadata": {},
   "source": [
    "Transformers is a Python library that makes downloading and training state-of-the-art ML models easy. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond.\n",
    "\n",
    "Two big strengths of this library are, one, it easily integrates with Hugging Face‚Äôs (previously mentioned) Models, Datasets, and Spaces repositories, and two, the library supports other popular ML frameworks such as PyTorch and TensorFlow.\n",
    "\n",
    "This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13ea93e-9945-4eda-a4e5-4b1db1cd5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import Conversation, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8a732-a317-4128-b17f-72b52af0d772",
   "metadata": {},
   "source": [
    "## 2.1. `Pipeline()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be71ce-306b-45a0-80cb-70413940f969",
   "metadata": {},
   "source": [
    "The easiest way to start using the library is via the `pipeline()` function, which abstracts NLP (and other) tasks into 1 line of code. For example, if we want to do sentiment analysis, we would need to select a model, tokenize the input text, pass it through the model, and decode the numerical output to determine the sentiment label (positive or negative).\n",
    "\n",
    "While this may seem like a lot of steps, we can do all this in 1 line via the `pipeline()` function, as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a582ecb-1284-428f-b6de-37b48c500f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998745918273926}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task=\"sentiment-analysis\")(\"Love this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691988-e274-4f5e-9f5b-2d3bf44b1cff",
   "metadata": {},
   "source": [
    "Of course, sentiment analysis is not the only thing we can do here. Almost any NLP task can be done in this way e.g. summarization, translation, question-answering, feature extraction (i.e. text embedding), text generation, zero-shot-classification, and more ‚Äî for a full list of the built-in tasks, check out the [pipleine() documentation](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task).\n",
    "\n",
    "In the above example code, since we did not specify a model, the default model for sentiment analysis was used (i.e. distilbert-base-uncased-finetuned-sst-2-english). However, if we wanted to be more explicit, we could have used the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1458a1b4-4e77-490c-b77a-0fe8c73dd099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998745918273926}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\n",
    "    task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")(\"Love this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a780a-3c31-45a7-8b9a-08195bb3774f",
   "metadata": {},
   "source": [
    "One of the greatest benefits of the Transformers library is we could have just as easily used any of the 28,000+ text classification models on Hugging Face‚Äôs [Models repository](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending) by simply changing the model name passed into the `pipeline()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60722c5f-e44d-4cac-80a2-a837f03b79a6",
   "metadata": {},
   "source": [
    "## 2.2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3476e-0c96-4077-b749-683bbaed63de",
   "metadata": {},
   "source": [
    "There is a massive repository of pre-trained models available on [Hugging Face](https://huggingface.co/models) (277,528 at the time of writing this). Almost all these models can be easily used via Transformers, using the same syntax we saw in the above code block.\n",
    "\n",
    "However, the models on Hugging Face aren‚Äôt only for the Transformers library. There are models for other popular machine learning frameworks e.g. PyTorch, Tensorflow, Jax. This makes Hugging Face‚Äôs Models repository useful to ML practitioners beyond the context of the Transformers library.\n",
    "\n",
    "To see what navigating the repository looks like, let‚Äôs consider an example. Say we want a model that can do text generation, but we want it to be available via the Transformers library so we can use it in one line of code (as we did above). We can easily view all models that fit these criteria using the ‚ÄúTasks‚Äù and ‚ÄúLibraries‚Äù filters.\n",
    "\n",
    "A model that meets these criteria is the newly released Llama 2. More specifically, `Llama-2‚Äì7b-chat-hf`, which is a model in the Llama 2 family with about 7 billion parameters, optimized for chat, and in the Hugging Face Transformers format. We can get more information about this model via its model card, which is shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6750699-017f-4c95-8a27-2ab9eb06710e",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure_1.webp\" style=\"width:800px;height:300px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f188e-a34e-4bde-935a-1106f21b335a",
   "metadata": {},
   "source": [
    "# 3. Installing ü§óTransformers (with Conda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5fb94-e9df-4994-a886-5777be44d7db",
   "metadata": {},
   "source": [
    "Now that we have a basic idea of the resources offered by Hugging Face and the Transformers library let‚Äôs see how we can use them. We start by installing the library and other dependencies.\n",
    "\n",
    "Hugging Face provides an [installation guide](https://huggingface.co/docs/transformers/installation) on its website. So, I won‚Äôt try to (poorly) duplicate that guide here. However, I will provide a quick 2-step guide on how to set up the conda environment for the example code below.\n",
    "\n",
    "Step 1) The first step is to download the hf-env.yml file available at the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face). You can either download the file directly or clone the whole repo.\n",
    "\n",
    "Step 2) Next, in your terminal (or anaconda command prompt), you can create a new conda environment based on hf-env.yml using the following commands"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b60f32d0-4653-4d19-836f-aa05f7c3ea90",
   "metadata": {},
   "source": [
    "conda env create --file hf-env.yml  -> didn't work for me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba1385-e5e4-4db8-b098-11e970f7720c",
   "metadata": {},
   "source": [
    "This may take a couple of minutes to install, but once it‚Äôs complete, you should be ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63858b-a2c1-4a56-99b2-ad9db7ec4723",
   "metadata": {},
   "source": [
    "# 4. Example Code: NLP with ü§óTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb732287-f950-4a68-9644-361f833b534e",
   "metadata": {},
   "source": [
    "With the necessary libraries installed, let‚Äôs jump into some example code. Here we will survey 3 NLP use cases, namely, sentiment analysis, summarization, and conversational text generation, using the pipeline() function.\n",
    "\n",
    "Toward the end, we will use Gradio to quickly generate a User Interface (UI) for any of these use cases and deploy it as an app on Hugging Face Spaces. All example code is available on the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816d4b8-f48a-48e4-a824-cd7357b0fa3b",
   "metadata": {},
   "source": [
    "## 4.1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc41487-cd41-4916-9b98-4ee95b6be8e7",
   "metadata": {},
   "source": [
    "We start sentiment analysis. Recall from earlier when we used the pipeline function to do something like the code block below, where we create a classifier that can label the input text as being either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80047fb7-9439-4036-abba-095163ced856",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86794202-b831-4f24-bbcb-5c6560de147f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997110962867737}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Hate this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9478fe-2fe6-4e2c-868d-3d061e474628",
   "metadata": {},
   "source": [
    "To go one step further, instead of processing text one by one, we can pass a list to the classifier to process as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e857a018-6173-4e8e-aee4-c4c2be981a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"This is great\",\n",
    "    \"Thanks for nothing\",\n",
    "    \"You've got to work on your face\",\n",
    "    \"You're beautiful, never change!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865a3d3c-70b2-4ea4-add7-6964f47f09dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998785257339478},\n",
       " {'label': 'POSITIVE', 'score': 0.9680059552192688},\n",
       " {'label': 'NEGATIVE', 'score': 0.877612292766571},\n",
       " {'label': 'POSITIVE', 'score': 0.9998120665550232}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8158a-4e0e-4396-b8db-d2c1f529725b",
   "metadata": {},
   "source": [
    "However, the text classification models on Hugging Face are not limited to just positive-negative sentiment. For example, the `roberta-base-go_emotions` model by SamLowe generates a suite of class labels. We can just as easily apply this model to text, as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc28a55-fdbf-4a55-a3bd-3abdd686e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd1cef5-19c6-4879-ad55-8db515946c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'admiration', 'score': 0.9526104927062988},\n",
       "  {'label': 'approval', 'score': 0.030472081154584885},\n",
       "  {'label': 'neutral', 'score': 0.015236252918839455},\n",
       "  {'label': 'excitement', 'score': 0.006063767243176699},\n",
       "  {'label': 'gratitude', 'score': 0.005296214483678341},\n",
       "  {'label': 'joy', 'score': 0.0044752005487680435},\n",
       "  {'label': 'curiosity', 'score': 0.004322339780628681},\n",
       "  {'label': 'realization', 'score': 0.004089609254151583},\n",
       "  {'label': 'optimism', 'score': 0.004077214282006025},\n",
       "  {'label': 'disapproval', 'score': 0.004076546523720026},\n",
       "  {'label': 'annoyance', 'score': 0.003528754459694028},\n",
       "  {'label': 'surprise', 'score': 0.0029730673413723707},\n",
       "  {'label': 'disappointment', 'score': 0.0027346329297870398},\n",
       "  {'label': 'love', 'score': 0.0026945793069899082},\n",
       "  {'label': 'amusement', 'score': 0.0024867462925612926},\n",
       "  {'label': 'confusion', 'score': 0.0023607409093528986},\n",
       "  {'label': 'pride', 'score': 0.0021013221703469753},\n",
       "  {'label': 'sadness', 'score': 0.001773047144524753},\n",
       "  {'label': 'anger', 'score': 0.0017196964472532272},\n",
       "  {'label': 'caring', 'score': 0.0013670100597664714},\n",
       "  {'label': 'desire', 'score': 0.0010478701442480087},\n",
       "  {'label': 'disgust', 'score': 0.0009689937578514218},\n",
       "  {'label': 'fear', 'score': 0.0005249762325547636},\n",
       "  {'label': 'relief', 'score': 0.0004862099594902247},\n",
       "  {'label': 'embarrassment', 'score': 0.00034175071050412953},\n",
       "  {'label': 'grief', 'score': 0.00033891823841258883},\n",
       "  {'label': 'remorse', 'score': 0.0002780948707368225},\n",
       "  {'label': 'nervousness', 'score': 0.0002078833058476448}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(text_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fa5be-e318-40df-83f2-68b8e0833202",
   "metadata": {},
   "source": [
    "## 4.2. Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa66f79-241d-46e0-9033-7db80d5481cd",
   "metadata": {},
   "source": [
    "Another way we can use the `pipeline()` function is for text summarization. Although this is an entirely different task than sentiment analysis, the syntax is almost identical.\n",
    "\n",
    "We first load in a summarization model. Then pass in some text along with a couple of input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d496ccf4-7ade-4512-bf78-be50f12949ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc006974-e313-4e73-934f-b1cfa7addc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hugging Face is an AI company that has become a major hub for open-source machine learning. \n",
    "Their platform has 3 major elements which allow users to access and share machine learning resources. \n",
    "First, is their rapidly growing repository of pre-trained open-source machine learning models for things such as natural language processing (NLP), computer vision, and more. \n",
    "Second, is their library of datasets for training machine learning models for almost any task. \n",
    "Third, and finally, is Spaces which is a collection of open-source ML apps.\n",
    "\n",
    "The power of these resources is that they are community generated, which leverages all the benefits of open source i.e. cost-free, wide diversity of tools, high quality resources, and rapid pace of innovation. \n",
    "While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem‚Äîtheir Transformers library.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b32cf4d-2631-419c-be2f-67fe09990040",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = summarizer(text, min_length=5, max_length=140)  # Takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c0869f-99f0-42e7-9c7a-03237c414a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Hugging Face is an AI company that has become a major hub for open-source machine learning. They have 3 major elements which allow users to access and share machine learning resources.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b19aab9-18d6-4fd2-ac87-18314e349cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face is an AI company that has become a major hub for open-source machine learning. They have 3 major elements which allow users to access and share machine learning resources.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5aef5-be9c-4ea5-8cde-77b9e5b1b81a",
   "metadata": {},
   "source": [
    "For more sophisticated use cases, it may be necessary to use multiple models in succession. For example, we can apply sentiment analysis to the summarized text to speed up the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fd5ff3f-5099-4990-8aea-dcf127889554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'neutral', 'score': 0.9101782441139221},\n",
       "  {'label': 'approval', 'score': 0.08781391382217407},\n",
       "  {'label': 'realization', 'score': 0.02325628511607647},\n",
       "  {'label': 'annoyance', 'score': 0.006623773369938135},\n",
       "  {'label': 'admiration', 'score': 0.004981071222573519},\n",
       "  {'label': 'disapproval', 'score': 0.004730117507278919},\n",
       "  {'label': 'optimism', 'score': 0.0033590674865990877},\n",
       "  {'label': 'disappointment', 'score': 0.002619002480059862},\n",
       "  {'label': 'confusion', 'score': 0.0019539794884622097},\n",
       "  {'label': 'excitement', 'score': 0.0012416992103680968},\n",
       "  {'label': 'disgust', 'score': 0.0011407819110900164},\n",
       "  {'label': 'joy', 'score': 0.001054016058333218},\n",
       "  {'label': 'amusement', 'score': 0.0009572406997904181},\n",
       "  {'label': 'love', 'score': 0.0008871030295267701},\n",
       "  {'label': 'desire', 'score': 0.0008553254301659763},\n",
       "  {'label': 'curiosity', 'score': 0.000826105650048703},\n",
       "  {'label': 'anger', 'score': 0.0007336378912441432},\n",
       "  {'label': 'caring', 'score': 0.0006971110124140978},\n",
       "  {'label': 'sadness', 'score': 0.0006702444516122341},\n",
       "  {'label': 'gratitude', 'score': 0.0006645758403465152},\n",
       "  {'label': 'surprise', 'score': 0.0005773786688223481},\n",
       "  {'label': 'relief', 'score': 0.00047890652786009014},\n",
       "  {'label': 'fear', 'score': 0.00045125061296857893},\n",
       "  {'label': 'pride', 'score': 0.00035174310323782265},\n",
       "  {'label': 'embarrassment', 'score': 0.00032615428790450096},\n",
       "  {'label': 'remorse', 'score': 0.00019475759472697973},\n",
       "  {'label': 'nervousness', 'score': 0.00018521567108109593},\n",
       "  {'label': 'grief', 'score': 0.00016037857858464122}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(summarized_text[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829333a-f929-4801-a6e3-4d45226a804f",
   "metadata": {},
   "source": [
    "## 4.3. Conversational"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe025ee-bfd4-445d-9006-b9f7610b26dd",
   "metadata": {},
   "source": [
    "Finally, we can use models developed specifically to generate conversational text. Since conversations require past prompts and responses to be passed to subsequent model responses, the syntax is a little different here. However, we start by instantiating our model using the `pipeline()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee4f8e90-c5e9-46e5-a1b7-065be48b5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6debe3b-7240-419e-816b-89f3dd489f53",
   "metadata": {},
   "source": [
    "Next, we can use the `Conversation()` class to handle the back-and-forth. We initialize it with a user prompt, then pass it into the chatbot model from the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edeba42b-be3c-4e7f-90bd-8b8ea9ef45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de3e3d3-0444-41bf-a735-7e9c4e0938c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(\"Hi I'm Shaw, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383629ad-1e0d-4e08-9842-8dd1251866a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 796d8ffb-5a5c-44cc-9b83-48aa90552ea7 \n",
       "user >> Hi I'm Shaw, how are you? "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce545ec-87a0-469c-8623-9d2fc1efe7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = chatbot(conversation)  # takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e49254b4-61b5-45e2-9466-bfad7df33feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 796d8ffb-5a5c-44cc-9b83-48aa90552ea7 \n",
       "user >> Hi I'm Shaw, how are you? \n",
       "bot >>  I'm doing well. How are you doing this evening? I just got home from work. "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f319a5a-6640-4b36-8cc2-5aec10d81f0e",
   "metadata": {},
   "source": [
    "To keep the conversation going, we can use the `add_user_input()` method to add another prompt to the conversation. We then pass the conversation object back into the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05e62bf2-a309-4463-b592-0351b16c7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_user_input(\"Where do you work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287c6690-8774-43f5-a5bf-123a12f6d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 796d8ffb-5a5c-44cc-9b83-48aa90552ea7 \n",
       "user >> Hi I'm Shaw, how are you? \n",
       "bot >>  I'm doing well. How are you doing this evening? I just got home from work. \n",
       "user >> Where do you work? "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed917cbd-168f-4020-a2af-febfda998167",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = chatbot(conversation)  # takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9497e6b8-1133-4cc5-9f3e-21523de1e511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 796d8ffb-5a5c-44cc-9b83-48aa90552ea7 \n",
       "user >> Hi I'm Shaw, how are you? \n",
       "bot >>  I'm doing well. How are you doing this evening? I just got home from work. \n",
       "user >> Where do you work? \n",
       "bot >>  I work at a grocery store. What about you? What do you do for a living? "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f78ccaa-d820-4e82-8aa2-cd22f32c5aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I'm doing well. How are you doing this evening? I just got home from work.\",\n",
       " ' I work at a grocery store. What about you? What do you do for a living?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.generated_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c068948-8795-4c21-8fd2-95aa944cbdd2",
   "metadata": {},
   "source": [
    "## 4.4. Chatbot UI with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3eb56-aa6a-4193-bee4-868f767841ab",
   "metadata": {},
   "source": [
    "While we get the base chatbot functionality with the Transformer library, this is an inconvenient way to interact with a chatbot. To make the interaction a bit more intuitive, we can use Gradio to spin up a front end in a few lines of Python code.\n",
    "\n",
    "This is done with the code shown below. At the top, we initialize two lists to store user messages and model responses, respectively. Then we define a function that will take the user prompt and generate a chatbot output. Next, we create the chat UI using the Gradio ChatInterface() class. Finally, we launch the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d789ffb-f0cb-4edf-a5cb-3f22b22dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = []\n",
    "response_list = []\n",
    "\n",
    "\n",
    "def vanilla_chatbot(message, history):\n",
    "    conversation = Conversation(\n",
    "        text=message, past_user_inputs=message_list, generated_responses=response_list\n",
    "    )\n",
    "    conversation = chatbot(conversation)\n",
    "\n",
    "    return conversation.generated_responses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b95b361-f490-4aaf-b43e-f946c545a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chatbot = gr.ChatInterface(\n",
    "    vanilla_chatbot,\n",
    "    title=\"Vanilla Chatbot\",\n",
    "    description=\"Enter text to start chatting.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2efbbd79-8017-4b15-a937-bccfbc22798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426b04f-61d3-443a-8bb9-17a5aed40d76",
   "metadata": {},
   "source": [
    "This will spin up the UI via a local URL. If the window does not open automatically, you can copy and paste the URL directly into your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742002ba-32c0-4a42-999f-bd2f82193610",
   "metadata": {},
   "source": [
    "## 4.5. Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f9c6b-5b73-4529-bb48-798d080cf165",
   "metadata": {},
   "source": [
    "To go one step further, we can quickly deploy this UI via Hugging Face Spaces. These are Git repositories hosted by Hugging Face and augmented by computational resources. Both free and paid options are available depending on the use case. Here we will stick with the free option.\n",
    "\n",
    "To make a new Space, we first go to the [Spaces page](https://huggingface.co/spaces) and click ‚ÄúCreate new space‚Äù. Then, configure the Space by giving it the name e.g. ‚Äúmy-first-space‚Äù and selecting Gradio as the SDK. Then hit ‚ÄúCreate Space‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac735aa-51fb-47d0-ae0f-d1dd69a5a536",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure_2.webp\" style=\"width:600px;height:800px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb77307-94e4-45ce-b3b3-a2e32bcff71b",
   "metadata": {},
   "source": [
    "Next, we need to upload app.py and requirements.txt files to the Space. The app.py file houses the code we used to generate the Gradio UI, and the requirements.txt file specifies the app‚Äôs dependencies. The files for this example are available at the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space) and the [Hugging Face Space](https://huggingface.co/spaces/shawhin/my-first-space/tree/main).\n",
    "\n",
    "Finally, we push the code to the Space just like we would to GitHub. The end result is a public application hosted on Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "raw",
   "id": "703da4be-6cb8-4398-b14b-0fd14bc25230",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/johnwslee/my_first_chatbot  <- Address to My chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b44c46-6875-4c1a-9544-39f00b64dc11",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80bb43-fd04-4560-a294-26a08c452954",
   "metadata": {},
   "source": [
    "Hugging Face has become synonymous with open-source language models and machine learning. The biggest advantage of their ecosystem is it gives small-time developers, researchers, and tinkers access to powerful ML resources.\n",
    "\n",
    "While we covered a lot of material in this post, we‚Äôve only scratched the surface of what the Hugging Face ecosystem can do. In future articles of this series, we will explore more advanced use cases and cover how to fine-tune models using ü§óTransformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e4341-d3b3-42d3-8e09-2cb36f7e4fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama2]",
   "language": "python",
   "name": "conda-env-llama2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
