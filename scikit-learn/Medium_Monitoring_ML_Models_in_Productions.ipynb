{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da3940c-e727-499c-8e53-ab04e3df0dbc",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/towards-data-science/monitoring-machine-learning-models-in-production-why-and-how-13d07a5ff0c6) in Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31407a50-970c-407c-b7b2-14bf118e7d68",
   "metadata": {},
   "source": [
    "# #1 Monitor the model performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493756a-6514-4420-bd42-c6781c1b9c4a",
   "metadata": {},
   "source": [
    "To detect any sign of model degradation, one of the direct and effective ways is to keep track of the performance metrics over time.\n",
    "\n",
    "- [Performance metrics for regression model](https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b): R-squared, Root mean squared error (RMSE), and mean absolute error (MAE)\n",
    "- [Performance metrics for classification model](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226): Precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef887f-62fb-45e6-be69-d804f69113c5",
   "metadata": {},
   "source": [
    "These performance metrics collected from the initial deployment serve as benchmarks for ongoing monitoring and evaluation. Periodically reassessing them is crucial whenever a new batch of ground truth data is collected, such as upon completion of a marketing campaign. If the error metrics rise above a predefined threshold, or if the metrics such as R-squared fall below the threshold, it is necessary to consider re-executing the data engineering process and retraining the model.\n",
    "\n",
    "While this monitoring approach provides valuable insights into any drifts, it tends to lag. We can take a more proactive approach to monitor the latest input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08b359-a4b3-46e4-b36a-6d179abaa259",
   "metadata": {},
   "source": [
    "# #2 Detect the changes in data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d4b4f-43c9-45da-8cba-b8ee40bc16ec",
   "metadata": {},
   "source": [
    "Instead of waiting for sufficient input data for reliable evaluation of model performance, we can apply statistical methods for comparing the data distributions of two datasets. In our case, we can determine if the distribution of the training dataset is the same as that of the latest production dataset. If it is not statistically confident that two distributions are the same, it suggests a drift in the model. This is as a proxy for performance changes.\n",
    "\n",
    "- Kolmogorov-Smirnov Test ([K-S Test](https://towardsdatascience.com/kolmogorov-smirnov-test-84c92fb4158d)): nonparametric test (i.e. with no assumption on the underlying data distribution) **for numeric features**, it is more sensitive near the center of the distribution than at the tails.\n",
    "\n",
    "> Interpretation: When p-value < 0.05, an alert indicating the presence of a drift is triggered.\n",
    "\n",
    "- Population Stability Index ([PSI](https://towardsdatascience.com/psi-and-csi-top-2-model-monitoring-metrics-924a2540bed8)): A statistical test that can be applied to **both numeric and categorical variables**. It is a metric that shows how each variable has diverged independently from the baseline values. It is sometimes called the Characteristic Stability Index (CSI) when evaluating the distribution of features rather than target variables.\n",
    "\n",
    "> Interpretation: A value 0~0.1 means no significant distribution change; a value 0.1~0.2 considered a moderate distribution change; and a value larger than 0.2 interpreted as a significant distribution change.\n",
    "\n",
    "Other famous statistical tests include [Kullback-Leibler divergence](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254), [Jensen-Shannon divergence](https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6), and [Wasserstein distance](https://towardsdatascience.com/the-gromov-wasserstein-distance-835c39d4751d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd3250-edfd-4f69-89ae-a91378ad5ec8",
   "metadata": {},
   "source": [
    "# #3 Monitor drift using a sliding window approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f50f2-9bdb-4275-bf65-9ee70bb4419d",
   "metadata": {},
   "source": [
    "Any delay in detecting data or pattern drifts results in a time gap, potentially leading to discrepancies between the ground truth and model predictions. To bridge the gap, are there any further advancements available? One promising idea is leveraging streaming data instead of batch data.\n",
    "\n",
    "The Adaptive Windowing ([ADWIN](https://scikit-multiflow.readthedocs.io/en/stable/api/generated/skmultiflow.drift_detection.ADWIN.html)) algorithm utilizes a sliding window approach to effectively detect concept drift. Unlike traditional fixed-size windows, ADWIN decides the size of the window by cutting the statistics window at different points. Whenever newly arriving data comes, ADWIN analyses the statistics and identifies the point at which two sub-windows demonstrate notable differences in their means.\n",
    "\n",
    "> Interpretation: When the absolute difference between the two means exceeds a pre-defined threshold, an alert indicating the presence of a drift is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5db79-cb5d-41a0-873e-2947074f422a",
   "metadata": {},
   "source": [
    "# Example Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2651a3-a621-4934-9281-43822056ee6d",
   "metadata": {},
   "source": [
    "We will utilize a [dataset](https://www.kaggle.com/datasets/parisrohan/credit-score-classification?datasetId=2289007&sortBy=voteCount&select=train.csv) obtained from Kaggle. The dataset comprises 100k records, encompassing 28 features that describe the customer demographics and their credit-related history.\n",
    "\n",
    "Our objective is to segregate customers in a global financial company into credit score brackets. The target variable is Credit_Score, a categorical measure classified as ‘Poor’, ‘Standard’, and ‘Good’.\n",
    "\n",
    "Examples of features include:\n",
    "\n",
    "- Occupation: Occupation of the customers (e.g. scientist, teacher, engineer, etc.)\n",
    "- Annual_Income: Annual income of the customers\n",
    "- Credit_History_Age: The age of customers’ credit history\n",
    "- Payment_Behaviour: 6 groups of payment behaviors based on spending frequency (low/ high) and payment amount (small/ medium/ large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7a0a4-0bab-4827-8230-cbe82c587113",
   "metadata": {},
   "source": [
    "# Model performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50904894-b297-45ed-8578-01b06fd58c7d",
   "metadata": {},
   "source": [
    "Let’s start by applying data cleansing and transformation techniques, including but not limited to:\n",
    "\n",
    "- Correcting/Removing records with missing values or incorrect data (e.g. age < 0), or duplicates\n",
    "- Detecting and handling data outliers\n",
    "- Performing min-max scaling on numeric variables\n",
    "- Applying label encoding on categorical variables\n",
    "\n",
    "*This part requires in-depth exploratory data analysis. However, since the focus is on sharing the monitoring strategies, the transformations performed are not discussed in detail here.*\n",
    "\n",
    "Afterward, we split the dataset and developed the advanced gradient boosting algorithm, LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8156c230-04b4-4a62-bbd6-62c140416f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749a8fc2-1e35-404a-b720-1441ac712391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ml_monitoring_example/train.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78afa0f-32ec-465b-ba24-729e701d8c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Month</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SSN</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Monthly_Inhand_Salary</th>\n",
       "      <th>Num_Bank_Accounts</th>\n",
       "      <th>...</th>\n",
       "      <th>Credit_Mix</th>\n",
       "      <th>Outstanding_Debt</th>\n",
       "      <th>Credit_Utilization_Ratio</th>\n",
       "      <th>Credit_History_Age</th>\n",
       "      <th>Payment_of_Min_Amount</th>\n",
       "      <th>Total_EMI_per_month</th>\n",
       "      <th>Amount_invested_monthly</th>\n",
       "      <th>Payment_Behaviour</th>\n",
       "      <th>Monthly_Balance</th>\n",
       "      <th>Credit_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1602</td>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>January</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23</td>\n",
       "      <td>821-00-0265</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>_</td>\n",
       "      <td>809.98</td>\n",
       "      <td>26.822620</td>\n",
       "      <td>22 Years and 1 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>80.41529543900253</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>312.49408867943663</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1603</td>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>February</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23</td>\n",
       "      <td>821-00-0265</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.944960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>118.28022162236736</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>284.62916249607184</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1604</td>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>March</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>-500</td>\n",
       "      <td>821-00-0265</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>28.609352</td>\n",
       "      <td>22 Years and 3 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>81.699521264648</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>331.2098628537912</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1605</td>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>April</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23</td>\n",
       "      <td>821-00-0265</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.377862</td>\n",
       "      <td>22 Years and 4 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>199.4580743910713</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>223.45130972736786</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1606</td>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>May</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23</td>\n",
       "      <td>821-00-0265</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>24.797347</td>\n",
       "      <td>22 Years and 5 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>41.420153086217326</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>341.48923103222177</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID Customer_ID     Month           Name   Age          SSN Occupation  \\\n",
       "0  0x1602   CUS_0xd40   January  Aaron Maashoh    23  821-00-0265  Scientist   \n",
       "1  0x1603   CUS_0xd40  February  Aaron Maashoh    23  821-00-0265  Scientist   \n",
       "2  0x1604   CUS_0xd40     March  Aaron Maashoh  -500  821-00-0265  Scientist   \n",
       "3  0x1605   CUS_0xd40     April  Aaron Maashoh    23  821-00-0265  Scientist   \n",
       "4  0x1606   CUS_0xd40       May  Aaron Maashoh    23  821-00-0265  Scientist   \n",
       "\n",
       "  Annual_Income  Monthly_Inhand_Salary  Num_Bank_Accounts  ...  Credit_Mix  \\\n",
       "0      19114.12            1824.843333                  3  ...           _   \n",
       "1      19114.12                    NaN                  3  ...        Good   \n",
       "2      19114.12                    NaN                  3  ...        Good   \n",
       "3      19114.12                    NaN                  3  ...        Good   \n",
       "4      19114.12            1824.843333                  3  ...        Good   \n",
       "\n",
       "   Outstanding_Debt Credit_Utilization_Ratio     Credit_History_Age  \\\n",
       "0            809.98                26.822620  22 Years and 1 Months   \n",
       "1            809.98                31.944960                    NaN   \n",
       "2            809.98                28.609352  22 Years and 3 Months   \n",
       "3            809.98                31.377862  22 Years and 4 Months   \n",
       "4            809.98                24.797347  22 Years and 5 Months   \n",
       "\n",
       "   Payment_of_Min_Amount Total_EMI_per_month Amount_invested_monthly  \\\n",
       "0                     No           49.574949       80.41529543900253   \n",
       "1                     No           49.574949      118.28022162236736   \n",
       "2                     No           49.574949         81.699521264648   \n",
       "3                     No           49.574949       199.4580743910713   \n",
       "4                     No           49.574949      41.420153086217326   \n",
       "\n",
       "                  Payment_Behaviour     Monthly_Balance Credit_Score  \n",
       "0   High_spent_Small_value_payments  312.49408867943663         Good  \n",
       "1    Low_spent_Large_value_payments  284.62916249607184         Good  \n",
       "2   Low_spent_Medium_value_payments   331.2098628537912         Good  \n",
       "3    Low_spent_Small_value_payments  223.45130972736786         Good  \n",
       "4  High_spent_Medium_value_payments  341.48923103222177         Good  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bef689-e99b-47c8-a13f-650c8a5d95ee",
   "metadata": {},
   "source": [
    "The following codes for data preprocessing is from [Kaggle](https://www.kaggle.com/code/supawitongkariyapong/project-credit-score-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339dd5d4-d1f4-49bc-b9ec-b9da6072785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column which is out of model scope\n",
    "d_col = [\n",
    "    \"ID\",\n",
    "    \"Customer_ID\",\n",
    "    \"Month\",\n",
    "    \"Name\",\n",
    "    \"SSN\",\n",
    "    \"Monthly_Inhand_Salary\",\n",
    "    \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\",\n",
    "    \"Interest_Rate\",\n",
    "    \"Num_of_Loan\",\n",
    "    \"Type_of_Loan\",\n",
    "    \"Changed_Credit_Limit\",\n",
    "    \"Num_Credit_Inquiries\",\n",
    "    \"Credit_Mix\",\n",
    "    \"Credit_Utilization_Ratio\",\n",
    "    \"Amount_invested_monthly\",\n",
    "    \"Occupation\",\n",
    "]\n",
    "df = df.drop(d_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e947976-0393-470f-bbbe-fb9a170b757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with nan\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b68ef9-0c36-4d91-b221-19eaac0f3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the incorrect data\n",
    "# df = df[df[\"Occupation\"].str.contains(\"_______\") == False]\n",
    "df = df[df[\"Payment_Behaviour\"].str.contains(\"!@9#%8\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6ebe2f-0e35-4503-9715-731b1c9b1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise the incorrect data whole table\n",
    "sym = \"\\\\`*_{}[]()>#@+!$:;\"\n",
    "col_int = [\n",
    "    \"Age\",\n",
    "    \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\",\n",
    "    \"Outstanding_Debt\",\n",
    "    \"Total_EMI_per_month\",\n",
    "    \"Monthly_Balance\",\n",
    "    \"Annual_Income\",\n",
    "]\n",
    "col_str = [\"Credit_History_Age\", \"Payment_of_Min_Amount\", \"Credit_Score\"]\n",
    "for i in col_int:\n",
    "    for c in sym:\n",
    "        df[i] = df[i].astype(str).str.replace(c, \"\")\n",
    "for i in col_str:\n",
    "    for c in sym:\n",
    "        df[i] = df[i].replace(c, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92face15-8606-4780-84d0-b4e1c438ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the information to the value\n",
    "df[\"Credit_History_Age\"] = (\n",
    "    df[\"Credit_History_Age\"].astype(str).str.replace(\" Years and \", \".\")\n",
    ")\n",
    "df[\"Credit_History_Age\"] = (\n",
    "    df[\"Credit_History_Age\"].astype(str).str.replace(\"Months\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c334666b-7ab4-4554-a03a-14f5f486330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the information to the value as level\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"Low_spent_Small_value_payments\", \"1\")\n",
    ")\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"Low_spent_Medium_value_payments\", \"2\")\n",
    ")\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"Low_spent_Large_value_payments\", \"3\")\n",
    ")\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"High_spent_Small_value_payments\", \"4\")\n",
    ")\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"High_spent_Medium_value_payments\", \"5\")\n",
    ")\n",
    "df[\"Payment_Behaviour\"] = (\n",
    "    df[\"Payment_Behaviour\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"High_spent_Large_value_payments\", \"6\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fafbd1cd-3500-4f82-a635-7581e2fdbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the object data the be float data type\n",
    "col_int2 = [\n",
    "    \"Age\",\n",
    "    \"Delay_from_due_date\",\n",
    "    \"Num_of_Delayed_Payment\",\n",
    "    \"Outstanding_Debt\",\n",
    "    \"Total_EMI_per_month\",\n",
    "    \"Monthly_Balance\",\n",
    "    \"Payment_Behaviour\",\n",
    "    \"Credit_History_Age\",\n",
    "    \"Annual_Income\",\n",
    "]\n",
    "for i in col_int2:\n",
    "    df[i] = df[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f382fc1-a6ed-4e5a-afa2-c88369edd0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to numeric\n",
    "df[\"Credit_Score\"] = df[\"Credit_Score\"].str.replace(\"Good\", \"3\", n=-1)\n",
    "df[\"Credit_Score\"] = df[\"Credit_Score\"].str.replace(\"Standard\", \"2\", n=-1)\n",
    "df[\"Credit_Score\"] = df[\"Credit_Score\"].str.replace(\"Poor\", \"1\", n=-1)\n",
    "df[\"Credit_Score\"] = df[[\"Credit_Score\"]].apply(pd.to_numeric)\n",
    "\n",
    "df[\"Payment_of_Min_Amount\"] = df[\"Payment_of_Min_Amount\"].str.replace(\"NM\", \"0\")\n",
    "df[\"Payment_of_Min_Amount\"] = df[\"Payment_of_Min_Amount\"].str.replace(\"Yes\", \"1\")\n",
    "df[\"Payment_of_Min_Amount\"] = df[\"Payment_of_Min_Amount\"].str.replace(\"No\", \"2\")\n",
    "df[\"Payment_of_Min_Amount\"] = df[[\"Payment_of_Min_Amount\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a957a822-a669-4fd7-a5fc-40bf00dab104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                       float64\n",
       "Annual_Income             float64\n",
       "Delay_from_due_date       float64\n",
       "Num_of_Delayed_Payment    float64\n",
       "Outstanding_Debt          float64\n",
       "Credit_History_Age        float64\n",
       "Payment_of_Min_Amount       int64\n",
       "Total_EMI_per_month       float64\n",
       "Payment_Behaviour         float64\n",
       "Monthly_Balance           float64\n",
       "Credit_Score                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5941fd4-a096-46aa-bf9f-7c65e357aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X = df.loc[:, df.columns != \"Credit_Score\"]\n",
    "Y = df[\"Credit_Score\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf7d50d0-ea6d-4649-b074-1aa80f5f26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LightGBM model\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(x_train, y_train)\n",
    "y_pred = lgbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08514099-0cf0-4362-8fa4-675a3e970758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.681\n",
      "Precision: 0.681\n",
      "Recall: 0.681\n"
     ]
    }
   ],
   "source": [
    "# Print performance metrics\n",
    "print(\"F1 score: %.3f\" % f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Precision: %.3f\" % precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128196c7-f8f6-4d36-ad9f-1d3885f9d134",
   "metadata": {},
   "source": [
    "During our model development, we gathered several performance metrics, including an F1 score of 0.810, a precision score of 0.818, and a recall score of 0.807. Subsequent monitoring can be evaluated similarly. For instance, if the F1 score falls below 0.75, it serves as an alert, prompting us to take immediate action to mitigate the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554171a-4247-4f84-9bab-7b287cae8687",
   "metadata": {},
   "source": [
    "# K-S Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3898b-262c-4f3f-b6c0-c511de6c8e9e",
   "metadata": {},
   "source": [
    "To demonstrate how the K-S Test works, I have selected a numeric feature `Credit_History_Age`. We will examine the sensitivity of this statistical test by creating three distinct datasets: one with 1000 samples, another with 5000 samples, and a third with approximately 64000 samples, which corresponds to the total size of the cleaned training samples. Each data point within the `Credit_History_Age` feature has been randomly picked from the training data and altered by a random floating-point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9fe133-a931-43fe-a9ca-57ed22e2ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6c75a1-ea40-4299-87e6-eee176ede0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new datasets with different no. of samples\n",
    "original_df = x_train[[\"Credit_History_Age\", \"Payment_Behaviour\"]].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "new_df = x_train[[\"Credit_History_Age\", \"Payment_Behaviour\"]].reset_index(drop=True)\n",
    "new_df1 = new_df.sample(n=1000).reset_index(drop=True)\n",
    "new_df2 = new_df.sample(n=5000).reset_index(drop=True)\n",
    "new_df3 = new_df.sample(n=len(x_train)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce55b996-660e-4ca4-9a50-ab3ee4d8bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare drifted data for numeric feature\n",
    "def drift_numeric_col(df, numeric_col, drift_range):\n",
    "    df[numeric_col] = df[numeric_col] + np.random.uniform(\n",
    "        0, drift_range, size=(df.shape[0],)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1bc0a25-1289-4fca-9edf-63b1b31f2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_numeric_col(new_df1, \"Credit_History_Age\", 2)\n",
    "drift_numeric_col(new_df2, \"Credit_History_Age\", 2)\n",
    "drift_numeric_col(new_df3, \"Credit_History_Age\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb6cc87c-6e8b-41d1-9921-02c773c9a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-S Test\n",
    "def ks_test(original_df, new_df, numeric_col):\n",
    "    test = stats.ks_2samp(original_df[numeric_col], new_df[numeric_col])\n",
    "    print(\"Column : %s , p-value : %1.3f\" % (numeric_col, test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94da6fdf-6ee0-434b-9f06-317bf8a18a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column : Credit_History_Age , p-value : 0.015\n",
      "Column : Credit_History_Age , p-value : 0.000\n",
      "Column : Credit_History_Age , p-value : 0.000\n"
     ]
    }
   ],
   "source": [
    "# Conduct K-S Test for numeric feature\n",
    "ks_test(original_df, new_df1, \"Credit_History_Age\")\n",
    "ks_test(original_df, new_df2, \"Credit_History_Age\")\n",
    "ks_test(original_df, new_df3, \"Credit_History_Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99ce15-c64b-44ee-9138-b70186dd135d",
   "metadata": {},
   "source": [
    "We obtained the following results:\n",
    "\n",
    "- For the dataset with 1000 samples, the p-value of the K-S Test is 0.093.\n",
    "- For the dataset with 5000 samples, the p-value of the K-S Test is 0.002.\n",
    "- For the dataset with around 64000 samples, the p-value of the K-S Test is 0.000.\n",
    "\n",
    "When examining the data drift scenario using 1000 samples, the p-value exceeded 0.05. Consequently, we can conclude that the two distributions remain similar. However, as the sample size increased to 5000 and beyond, the K-S Test exhibited exceptional performance, yielding p-values significantly lower than 0.05. This provides a clear indication of a data distribution drift, serving as a clear alert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8e360-e258-44ff-83e2-5486d4157de1",
   "metadata": {},
   "source": [
    "# PSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be306651-fe96-43ba-9e73-aa0bb4035075",
   "metadata": {},
   "source": [
    "In addition to utilizing the K-S Test, we will also leverage the PSI (Population Stability Index) to evaluate the numeric feature `Credit_History_Age` and assess the categorical feature `Payment_Behaviour`. To represent the drift effect, we have randomly replaced 80% of the data of the feature `Payment_Behaviour` with specific label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6415a78f-7c79-45dd-b618-6e0fb51a02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a4831b8-60f7-447a-b5e2-20f28c10e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare drifted data for categorical column\n",
    "def drift_cat_col(df, cat_col, drift_ratio):\n",
    "    no_of_drift = round(len(df) * drift_ratio)\n",
    "    random_numbers = [random.randint(0, 1) for _ in range(no_of_drift)]\n",
    "    indices = random.sample(range(len(df[cat_col])), no_of_drift)\n",
    "    df.loc[indices, cat_col] = random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d56bfdc-6bac-4775-bc2d-c67839948f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_cat_col(new_df1, \"Payment_Behaviour\", 0.8)\n",
    "drift_cat_col(new_df2, \"Payment_Behaviour\", 0.8)\n",
    "drift_cat_col(new_df3, \"Payment_Behaviour\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1009e65-feb7-40bd-acc2-e2c390d30dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(data_base, data_new, num_bins=10):\n",
    "    # Sort the data\n",
    "    data_base = sorted(data_base)\n",
    "    data_new = sorted(data_new)\n",
    "\n",
    "    # Prepare the bins\n",
    "    min_val = min(data_base[0], data_new[0])\n",
    "    max_val = max(data_base[-1], data_new[-1])\n",
    "    bins = [min_val + (max_val - min_val) * (i) / num_bins for i in range(num_bins + 1)]\n",
    "    bins[0] = min_val - 0.0001\n",
    "    bins[-1] = max_val + 0.0001\n",
    "\n",
    "    # Bucketize the baseline data and count the samples\n",
    "    bins_base = pd.cut(data_base, bins=bins, labels=range(1, num_bins + 1))\n",
    "    df_base = pd.DataFrame({\"base\": data_base, \"bin\": bins_base})\n",
    "    grp_base = df_base.groupby(\"bin\").count()\n",
    "    grp_base[\"percent_base\"] = grp_base[\"base\"] / grp_base[\"base\"].sum()\n",
    "\n",
    "    # Bucketize the new data and count the samples\n",
    "    bins_new = pd.cut(data_new, bins=bins, labels=range(1, num_bins + 1))\n",
    "    df_new = pd.DataFrame({\"new\": data_new, \"bin\": bins_new})\n",
    "    grp_new = df_new.groupby(\"bin\").count()\n",
    "    grp_new[\"percent_new\"] = grp_new[\"new\"] / grp_new[\"new\"].sum()\n",
    "\n",
    "    # Compare the bins\n",
    "    psi_df = grp_base.join(grp_new, on=\"bin\", how=\"inner\")\n",
    "\n",
    "    # Calculate the PSI\n",
    "    psi_df[\"percent_base\"] = psi_df[\"percent_base\"].replace(0, 0.0001)\n",
    "    psi_df[\"percent_new\"] = psi_df[\"percent_new\"].replace(0, 0.0001)\n",
    "    psi_df[\"psi\"] = (psi_df[\"percent_base\"] - psi_df[\"percent_new\"]) * np.log(\n",
    "        psi_df[\"percent_base\"] / psi_df[\"percent_new\"]\n",
    "    )\n",
    "\n",
    "    # Return the total PSI value\n",
    "    return np.sum(psi_df[\"psi\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b914f0d-9043-47d0-8797-fe28b6e35d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026888669992667112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct K-S Test for numeric feature\n",
    "psi(original_df[\"Credit_History_Age\"], new_df1[\"Credit_History_Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f1defbc-76b3-4aef-a22a-cc5a2b42650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03748601241124246"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(original_df[\"Credit_History_Age\"], new_df2[\"Credit_History_Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea320ab-e37b-4794-9a65-93a2ac1c81d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03182059320799732"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(original_df[\"Credit_History_Age\"], new_df3[\"Credit_History_Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528430d-0a03-45d6-9c0c-8b00a512e3bc",
   "metadata": {},
   "source": [
    "- Numeric feature Credit_History_Age\n",
    "With a sample size of 1000, the PSI value is 0.023.\n",
    "\n",
    "With a sample size of 5000, the PSI value is 0.015.\n",
    "\n",
    "With a sample size of approximately 64000, the PSI value is 0.021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3a8eb87-616e-43a8-b342-f5976ca58f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.325712516729441"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct K-S Test for categorical feature\n",
    "psi(original_df[\"Payment_Behaviour\"], new_df1[\"Payment_Behaviour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da960220-bcae-434e-bd88-14c06ecedc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.343405836146468"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(original_df[\"Payment_Behaviour\"], new_df2[\"Payment_Behaviour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a8b0dd5-f8bf-4795-8611-881597c193c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.373471810217242"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(original_df[\"Payment_Behaviour\"], new_df3[\"Payment_Behaviour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b6429-3ac6-45e1-92dd-f2eac1c91627",
   "metadata": {},
   "source": [
    "- Categorical feature Payment_Behaviour\n",
    "With a sample size of 1000, the PSI value is 0.108.\n",
    "\n",
    "With a sample size of 5000, the PSI value is 0.111.\n",
    "\n",
    "With a sample size of approximately 64000, the PSI value is 0.112."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221f51a-aede-4054-a576-04bdc76f1c6e",
   "metadata": {},
   "source": [
    "All the PSI values for the Credit_History_Age feature are significantly lower than 0.1, indicating no significant distribution change. By comparing these results with those obtained from the K-S Test, we observe that the K-S Test exhibits greater sensitivity in detecting distribution changes compared to PSI.\n",
    "\n",
    "On the other hand, the PSI values for the Payment_Behaviour feature are around 0.11, signifying a moderate distribution change. Interestingly, the three PSI values remain relatively consistent, implying that PSI’s effectiveness is less dependent on sample sizes. Besides, PSI has the flexibility to monitor across various feature types, positioning it still a valuable approach for drift detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0f1d6-827d-4ff2-8df4-7130615c8bd2",
   "metadata": {},
   "source": [
    "# ADWIN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e4b14-7c80-4c08-8075-a62ebbf9e084",
   "metadata": {},
   "source": [
    "Lastly, we test the power of ADWIN in detecting the change in the numeric feature Credit_History_Age. The [data stream](https://en.wikipedia.org/wiki/Streaming_data) consists of the training data, followed by the drifted data. We expect that the algorithm will promptly identify the drift shortly after examining the entirety of the original data.\n",
    "\n",
    "To visually represent the situation, a scatter plot is created to showcase the final 500 points of the original data in blue, followed by the initial 500 points of the drifted data, depicted in green. The drifted data exhibits a slightly higher average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96ca7207-eae3-4a2f-9d03-b1f72ecf323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.drift_detection import ADWIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f72bdf42-d0e1-412b-be2b-f1e2ff5b91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adwin = ADWIN()\n",
    "data_stream = []\n",
    "data_stream = np.concatenate(\n",
    "    (original_df[\"Credit_History_Age\"], new_df3[\"Credit_History_Age\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad853e1-ff01-494b-bb3b-92780d933b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stream elements to ADWIN and verify if drift occurred\n",
    "for i in range(len(data_stream)):\n",
    "    adwin.add_element(data_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        print('Change detected at index {}'.format(i))\n",
    "    adwin.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a216c-ee39-4260-a5a4-34995a546ee5",
   "metadata": {},
   "source": [
    "By continuously adding stream elements, ADWIN identifies the change at index 64457, which is the 637th data point within the drifted data. In comparison, the K-S Test and PSI necessitate a larger number of data points to conclude the presence of a drift confidently. The better performance of ADWIN is a testament to its capability to monitor diverse features at speed and ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079427e6-e793-4bdb-88ec-213f06abd9ff",
   "metadata": {},
   "source": [
    "# Wrapping it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c66819-5904-4581-9c27-89d95a49dc5a",
   "metadata": {},
   "source": [
    "We’ve delved into the crucial concepts of data drift and model drift, which can lead to model decay in production. We can proactively monitor and detect drift conditions using model performance metrics, statistical tests, and adaptive windowing techniques. Unlike participating in a one-time Kaggle competition, building a production-ready ML system is an iterative journey. It requires a mindset shift towards integrating comprehensive model monitoring to ensure a robust and consistently high-performance serving space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39b72e-7171-4d28-b32f-bcb94d20a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ls_env]",
   "language": "python",
   "name": "conda-env-ls_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
