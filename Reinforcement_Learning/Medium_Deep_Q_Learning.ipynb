{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094616e3-bd2c-4cc6-8342-474f8fa4ce1b",
   "metadata": {},
   "source": [
    "The following is from [this article](https://medium.com/@arshren/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d) in Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f8133-8f81-40a2-a570-e3388ea9a6ce",
   "metadata": {},
   "source": [
    "# 1. Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b6b6c-3974-4ae0-affe-9123350deb24",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning combines Reinforcement Learning algorithms with Artificial Neural Networks. It allows the agent to learn an optimal policy for sequential decision problems by maximizing the cumulative future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab059e-e21c-4049-a0c3-2568beabbd08",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a goal-oriented algorithm where the agent aims to find optimal actions for a given state in an environment to maximize the long-term reward.\n",
    "\n",
    "A Policy is a mapping of the states to actions that define the behavior of an agent in an environment. However, an Optimized Policy is a policy in which the agent is trained to maximize the cumulative reward over time.\n",
    "\n",
    "The goal of reinforcement learning is to find an optimized policy that maps optimal actions the agent takes for the different environmental states.\n",
    "\n",
    "The policies can be represented using a lookup table, linear functions, or neural networks depending on the complexity of the action space and state space for the environment. An optimal policy is derived by selecting the highest valued action in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a064dfd-5eb6-4ddb-8236-ac8a607ff09c",
   "metadata": {},
   "source": [
    "> For a low-dimensional state space and action space, a lookup table like a Q-table might be an excellent choice to represent a policy; however, when we have high-dimensional state spaces and action spaces, a neural network might be the option to learn optimized policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7c376-57f6-40d6-9591-eb9ac78fe136",
   "metadata": {},
   "source": [
    "Neural Networks are several layers of nodes that approximate the function represented by the input data. Hence, a neural network can approximate a value function or a policy function to map states to values or state-action pairs to Q values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3f961-2b5a-4bd2-8d15-cbf3b5a3e927",
   "metadata": {},
   "source": [
    "# 2. Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f38018-1c70-4ef5-8108-d2700f09d59c",
   "metadata": {},
   "source": [
    "Deep Q learning is a deep Q-network(DQN) that combines Q learning Reinforcement Learning with a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9455a4-4d59-4fdb-b3eb-7823074835e6",
   "metadata": {},
   "source": [
    "If the state of the environment is represented as a vector of numbers, then use a fully-connected neural network. However, when the input data has a grid-like structure, such as an image, then use the convolutional neural network, which uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f1893-0c48-400a-bc42-dacebe79bfa0",
   "metadata": {},
   "source": [
    "> Q-Learning is an off-policy temporal difference algorithm to find the optimal policy by updating the state-action value function(Q) at every step using the Bellman Optimality equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2f65b-7628-4e70-820b-3e678ae9d4c2",
   "metadata": {},
   "source": [
    "<img src=\"equ_2.webp\" style=\"width:800px;height:50px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2b0f0-c4c4-40dd-9eb5-bbc4d8d78451",
   "metadata": {},
   "source": [
    "Q-Learning, an off-policy algorithm, uses behavioral and target policies. A behavioral policy is used to explore the environment and to collect samples generating the agent’s behavior, and a target policy is learned, which is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed840f-f461-48c8-bf53-84394c0b2ae1",
   "metadata": {},
   "source": [
    "The initial state is fed as an input to the neural network and returns the Q-value of all possible actions as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db7013-3087-4a85-9af2-02e901261205",
   "metadata": {},
   "source": [
    "<img src=\"figure_3.webp\" style=\"width:800px;height:400px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899a3db-a5e3-410c-929e-6044e6de7990",
   "metadata": {},
   "source": [
    "# 3. Challenges using the Neural Network for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bfb52-cdaa-4628-8a4e-02db7f9d195c",
   "metadata": {},
   "source": [
    "Reinforcement Learning is known to be unstable or even to diverge when a neural network is used to represent the nonlinear function approximator or represent the Q function, also referred to as the action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62b3d3-0acb-455a-af89-d5a45bfb0578",
   "metadata": {},
   "source": [
    "The reason for instability is due to\n",
    "\n",
    "- Correlation between the Q-values and the target values: Correlations present in the sequence of observations, the fact that small updates may significantly change the policy and therefore change the data distribution,\n",
    "- Non-stationary target: There is a correlation between the action values (Q) and the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d32899-b546-44bd-93a9-e4b928f50c0b",
   "metadata": {},
   "source": [
    "The target value is not stationary and keeps changing with every iteration, unlike in a typical neural network where the target is fixed and does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcba36-6867-4202-acd1-8e16dcb6a95c",
   "metadata": {},
   "source": [
    "# 4. Addressing the instabilities in Deep Neural Networks for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a8cbd-30c1-414c-a764-c5bc19f4fc9f",
   "metadata": {},
   "source": [
    "The instabilities in a deep neural network for RL are resolved with a novel variant of Q-learning using two key ideas.\n",
    "\n",
    "1. Experience Play\n",
    "2. Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aabee-872b-496f-a1d9-5885bbcba655",
   "metadata": {},
   "source": [
    "## 4.1. Experience Play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78d7fc-c04b-4b92-97de-13be5c7fba36",
   "metadata": {},
   "source": [
    "Experience play is a technique to stabilize the training of the Q-function by allowing the agent to revisit and learn from past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98e62d-25a0-401d-a1e9-1653370600c4",
   "metadata": {},
   "source": [
    "The agent's experience: state, action, reward, and next state(Sₜ, Aₜ, Rₜ, Sₜ+₁) at each time step are stored in a replay memory. During the training, the agent randomly samples a batch of experiences from the replay memory to update the Q function.\n",
    "\n",
    "Experience play removes the correlation in the observation sequence and smoothes the changes in data distribution. It also allows the agent to learn from diverse experiences to stabilize the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5414320-8828-40c7-ae9d-b5acadfbab97",
   "metadata": {},
   "source": [
    "## 4.2. Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf6b53-ed6d-4f33-a1a7-284949c2653a",
   "metadata": {},
   "source": [
    "Target network is a copy of the Q-network, which is used to approximate the Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af356b98-14d6-4938-bcb9-4bb3b129eda0",
   "metadata": {},
   "source": [
    "The Target network maintains a separate weight vector θ⁻ to create a temporal gap between the target action-value function and the Q network's action-value function that is continually updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f6947-4a36-471c-8dc4-46d260ac948d",
   "metadata": {},
   "source": [
    "<img src=\"figure_4.webp\" style=\"width:800px;height:500px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7666d-d11d-4307-8326-d406b6965f72",
   "metadata": {},
   "source": [
    "The Target network parameters, θ⁻, are only updated with the Q-network parameters, θ, in every C step and are held fixed between individual updates. C is the period of time chosen as a hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983da376-485a-47f4-a1c4-deee9c887e29",
   "metadata": {},
   "source": [
    "<img src=\"equ_3.webp\" style=\"width:800px;height:50px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49ed0b-3b51-496f-b9ba-ebdd27f52a78",
   "metadata": {},
   "source": [
    "Choosing a separate Target network makes divergence unlikely as it adds a delay between the time the Q value is updated and the time the target Qᵀ values are updated.\n",
    "\n",
    "The Target network stabilizes the training process of the DQN by allowing the Target network to be relatively stable while incorporating the most recent changes to the Q Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda2917-3b7f-4f33-a14e-812baf8cc12e",
   "metadata": {},
   "source": [
    "# 5. Pytorch Implementation of Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac726b-d640-458e-b5a6-c165c674cd8a",
   "metadata": {},
   "source": [
    "We will train the [CartPole-V1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) using Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e4e5f-0f26-453f-81a6-2b89b55e34d2",
   "metadata": {},
   "source": [
    "- **Task** : The goal is for the agent to balance the pole by applying forces in the left and right direction on the cart where the pendulum is placed upright.\n",
    "\n",
    "- **Action space** : There are two actions the agent can take. 0:Pushcart to the left, and 1:Pushcart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86a6cc-7753-49e9-abec-4f58a178c1bd",
   "metadata": {},
   "source": [
    "<img src=\"table_1.webp\" style=\"width:800px;height:250px;background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837d83d-4d57-452e-a2e9-6b9f8dcb5b6a",
   "metadata": {},
   "source": [
    "- **Rewards** : The agent receives a reward of +1 for every time step that the pole remains upright on the cart, including the termination step. The threshold for rewards is 500 for v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c346e20a-1e9a-4921-a957-79d2cdc65c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827e6ae-7703-46df-aa67-b6fd7b5efc11",
   "metadata": {},
   "source": [
    "Setting up the device as well as setting the plot in the interactive mode of the matplotlib. This will allow the figures and plots to be open in a separate window and will be updated while the script runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705bb651-d4d0-47c3-a522-189ad083d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "\n",
    "# setting the device\n",
    "# if gpu is to be used for Mac OS\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# if gpu is to be used for cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11cdfd-5475-45a7-a9a6-6106ebdd3fc2",
   "metadata": {},
   "source": [
    "**Deep Q-learning uses Experience play** to stabilize the training by learning from the agent’s past experiences.\n",
    "\n",
    "The agent’s experience: state, action, reward, and next state(Sₜ, Aₜ, Rₜ, Sₜ+₁) at each time step are stored in a replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25de756c-8676-495d-9238-8f263afecd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of the Experiences to store\n",
    "Experience = namedtuple(\"Experience\", (\"state\", \"action\", \"next_state\", \"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e838f0d4-d455-4935-8b64-2d58a9fc0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the Experience Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # Save the Experience into memory\n",
    "        self.memory.append(Experience(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # selecting a random batch of Experience for training\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4fb02-a316-4b3b-a7a0-86d75e23791a",
   "metadata": {},
   "source": [
    "## 5.1. Building the DQN(Deep Q-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83db879-0bae-4fb9-ac65-382fc1f35d79",
   "metadata": {},
   "source": [
    "As the state of the cartPole-v1 environment is represented as a vector of numbers, we will use a fully-connected neural network with three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fa42fe-650d-4cc0-a28b-ad92c0752445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_observations: observation/state size of the environment\n",
    "        n_actions: number of discrete actions available in the environment\n",
    "        hidden_size: size of hidden layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac42370-f6d2-43ad-ad47-a4b00510fef2",
   "metadata": {},
   "source": [
    "## 5.2. Setting the values for the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9478bfd-34ef-4202-bac3-90c4c9965239",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = (\n",
    "    128  # BATCH_SIZE is the number of Experience sampled from the replay buffer\n",
    ")\n",
    "GAMMA = 0.99  # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPSILON_START = 0.9  # EPSILON_START is the starting value of epsilon\n",
    "EPSILON_END = 0.05  # EPSILON_END is the final value of epsilon\n",
    "EPSILON_DECAY = 1000  # EPSILON_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005  # TAU is the update rate of the target network\n",
    "LR = 1e-4  # LR is the learning rate of the AdamW optimizer\n",
    "HIDDEN_SIZE = 128  # the hidden layers in the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95e5b4-fa5e-43d2-aa7f-0f05bcc14297",
   "metadata": {},
   "source": [
    "## 5.3. Perform an Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f42a0-0d30-4078-8223-0b8349d41798",
   "metadata": {},
   "source": [
    "An action will be chosen based on an epsilon greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b45777-ec6a-42b6-b61f-dabee8848131",
   "metadata": {},
   "source": [
    "> Epsilon greedy policy allows the agent selects an action with the highest reward with a probability of 1-epsilon to exploit the best action for the state and selects a random action with a probability of epsilon to explore the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99398f3-15e1-4779-8328-a26a52b7a68c",
   "metadata": {},
   "source": [
    "The epsilon parameter controls the balance between exploration and exploitation.\n",
    "\n",
    "The probability of choosing a random action will start at `EPSILON_START` and decay exponentially towards `EPSILON_END`.\n",
    "\n",
    "`EPSILON_DECAY` controls the rate of decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2430876f-0996-4a5a-ab3f-d2439bb250ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(\n",
    "        -1.0 * steps_done / EPSILON_DECAY\n",
    "    )\n",
    "\n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        # max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was found\n",
    "        # so we pick action with the larger expected reward.\n",
    "        with torch.no_grad():\n",
    "            return policy_network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(\n",
    "            [[env.action_space.sample()]], device=device, dtype=torch.long\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47cb47dc-a6f3-47f4-9f5d-67e34f92af20",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 2x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 2x128)"
     ]
    }
   ],
   "source": [
    "policy_network(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73519e-01da-4d51-9b95-2e3f2042a827",
   "metadata": {},
   "source": [
    "## 5.4. Plot the duration of training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2d6efd-9f4c-4f13-81a0-0f2d883f3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to plot the durations of episodes, \n",
    "along with an average over the last 100 episodes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_scores(show_result=False):\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_duration, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training...\")\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(duration_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(duration_t) >= 100:\n",
    "        means = duration_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1f72e-d3d5-4b7e-9d0c-585e5369ea49",
   "metadata": {},
   "source": [
    "## 5.5. Optimize the model one single step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6f963-5fff-4c3a-9520-39f9a5c0d981",
   "metadata": {},
   "source": [
    "compute\n",
    "\n",
    "- Q(Sₜ, Aₜ): the actions the agent would have taken for each batch state according to policy_net\n",
    "- V(Sₜ+₁) : the target network compute maxₐQ(Sₜ+₁, a) for all next states for stability\n",
    "- Expected Q value: reward + (γ * V(Sₜ+₁))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaec67-f6c5-463a-a38d-3e7bc69f2002",
   "metadata": {},
   "source": [
    "Compute Huber loss which acts like the mean squared error when the error is small but acts like the mean absolute error when the error is large, making it more robust to outliers when the estimates of Q are very noisy\n",
    "\n",
    "The Target network has its weights kept frozen most of the time but is updated with the Policy network’s weights every so often. This is usually a set number of steps, but we shall use episodes for simplicity when training the DQN.\n",
    "\n",
    "The gradient clipping is performed to optimize the policy network. If the gradients of the parameters of the policy network exceed 100 in magnitude, they will be set to 100 (or -100 if they are negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d14a10-06a5-450e-b2b2-dbb3fc78482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transition = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    \"\"\"\n",
    "    # Transpose the batch and convert the  batch-array of Experience\n",
    "    # to Transition of batch-arrays\n",
    "    \"\"\"\n",
    "    batch = Experience(*zip(*transition))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch element\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a)\n",
    "    # These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(\n",
    "            1\n",
    "        )[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea628735-d1eb-45ce-b05a-db9ade35ed8c",
   "metadata": {},
   "source": [
    "## 5.6. Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d0ef6-5628-44c4-a3e2-3bb915fa70fd",
   "metadata": {},
   "source": [
    "The main training loop uses a fixed number of episodes and steps to train.\n",
    "\n",
    "In the beginning, reset the environment using `env.reset()` and obtain the initial `state` Tensor. Then, sample an action `choose_action(state)` to observe the next state and the reward.\n",
    "\n",
    "The training loop uses a replay buffer to store experiences, a Target network, and a Policy network. The Target network's parameters are updated with the Policy network's parameters.\n",
    "\n",
    "The agent will perform a soft update of the Target Network’s weights as it makes the Target Network’s weights converge to the Policy Network’s weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b1df1d-1da9-48a9-b8dc-ecbe1b10b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()[0]\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bac75fa-e3f9-42ba-8311-1df2df4cc940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network = DQN(n_observations, n_actions, HIDDEN_SIZE).to(device)\n",
    "target_network = DQN(n_observations, n_actions, HIDDEN_SIZE).to(device)\n",
    "# updates the parameters of the target network  with the parameters of the policy network\n",
    "target_network.load_state_dict(policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a1067c-3321-4653-bf7d-4dce77053cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "episode_duration = []\n",
    "num_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c97fce60-60ab-47d8-9b21-d723aa2d20f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 2x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# sample an action\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# execute it, observe the next screen and the reward\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m, in \u001b[0;36mchoose_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;241m>\u001b[39m epsilon_threshold:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# max(1) will return largest column value of each row.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# second column on max result is index of where max element was found\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# so we pick action with the larger expected reward.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     17\u001b[0m         [[env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()]], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     18\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ls_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 2x128)"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    # At the beginning we reset the environment and\n",
    "    # initialize the state Tensor.\n",
    "    state = env.reset()[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        # sample an action\n",
    "        action = choose_action(state)\n",
    "        # execute it, observe the next screen and the reward\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(\n",
    "                observation, dtype=torch.float32, device=device\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "        # Store the experience in the memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        # The agent  performs an optimization step on the Policy Network using the stored memory\n",
    "        optimize_model()\n",
    "\n",
    "        \"\"\"\n",
    "        The agent will perform a soft update of the Target Network's weights, \n",
    "        with the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \n",
    "        this helps to make the Target Network's weights converge to the Policy Network's weights.\n",
    "        \"\"\"\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # policy_network.state_dict() returns the parameters of the policy network\n",
    "        # target_network.load_state_dict() loads these parameters into the target network.\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        policy_net_state_dict = policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[\n",
    "                key\n",
    "            ] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            episode_duration.append(t + 1)\n",
    "            print(\n",
    "                \"Episode\",\n",
    "                i,\n",
    "                \"Game terminated after\",\n",
    "                t,\n",
    "                \"steps with reward\",\n",
    "                total_reward,\n",
    "            )\n",
    "\n",
    "            break\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989fbf7-30a4-4b6b-9783-de1728e80348",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d7ef8-4c6c-4926-9c92-6eb02f770c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ls_env]",
   "language": "python",
   "name": "conda-env-ls_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
